for basic implementation without hierarchical and undevieded between train test collab creation: (15.03) (all epochs)
######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
######UPSAMPLING DONE######
Value counts: {0.0: 673437, 1.0: 673437}
Iteration 1, loss = 0.59590037
Iteration 2, loss = 0.57230412
Iteration 3, loss = 0.56476635
Iteration 4, loss = 0.55986489
Iteration 5, loss = 0.55571970
Iteration 6, loss = 0.55270696
Iteration 7, loss = 0.55020309
Iteration 8, loss = 0.54810538
Iteration 9, loss = 0.54636048
Iteration 10, loss = 0.54477902
Iteration 11, loss = 0.54336258
Iteration 12, loss = 0.54209127
Iteration 13, loss = 0.54103860
Iteration 14, loss = 0.54008269
Iteration 15, loss = 0.53913056
Iteration 16, loss = 0.53836873
Iteration 17, loss = 0.53760910
Iteration 18, loss = 0.53695348
Iteration 19, loss = 0.53619574
Iteration 20, loss = 0.53569864
Iteration 21, loss = 0.53518920
Iteration 22, loss = 0.53482786
Iteration 23, loss = 0.53423857
Iteration 24, loss = 0.53390535
Iteration 25, loss = 0.53359432
Iteration 26, loss = 0.53302873
Iteration 27, loss = 0.53272768
Iteration 28, loss = 0.53232749
Iteration 29, loss = 0.53184499
Iteration 30, loss = 0.53140853
Iteration 31, loss = 0.53110440
Iteration 32, loss = 0.53087105
Iteration 33, loss = 0.53038741
Iteration 34, loss = 0.52981474
Iteration 35, loss = 0.52920273
Iteration 36, loss = 0.52856558
Iteration 37, loss = 0.52789534
Iteration 38, loss = 0.52716474
Iteration 39, loss = 0.52618169
Iteration 40, loss = 0.52551677
Iteration 41, loss = 0.52470327
Iteration 42, loss = 0.52373452
Iteration 43, loss = 0.52309757
Iteration 44, loss = 0.52257166
Iteration 45, loss = 0.52206841
Iteration 46, loss = 0.52150802
Iteration 47, loss = 0.52107965
Iteration 48, loss = 0.52053477
Iteration 49, loss = 0.52035718
Iteration 50, loss = 0.51994794
Iteration 51, loss = 0.51960620
Iteration 52, loss = 0.51918485
Iteration 53, loss = 0.51891875
Iteration 54, loss = 0.51866736
Iteration 55, loss = 0.51832148
Iteration 56, loss = 0.51807648
Iteration 57, loss = 0.51762976
Iteration 58, loss = 0.51733940
Iteration 59, loss = 0.51709530
Iteration 60, loss = 0.51679960
Iteration 61, loss = 0.51644947
Iteration 62, loss = 0.51619828
Iteration 63, loss = 0.51585148
Iteration 64, loss = 0.51568139
Iteration 65, loss = 0.51530978
Iteration 66, loss = 0.51514706
Iteration 67, loss = 0.51470918
Iteration 68, loss = 0.51460668
Iteration 69, loss = 0.51434271
Iteration 70, loss = 0.51421194
Iteration 71, loss = 0.51396676
Iteration 72, loss = 0.51359424
Iteration 73, loss = 0.51353069
Iteration 74, loss = 0.51338583
Iteration 75, loss = 0.51304115
Iteration 76, loss = 0.51284845
Iteration 77, loss = 0.51269872
Iteration 78, loss = 0.51258441
Iteration 79, loss = 0.51228184
Iteration 80, loss = 0.51206047
Iteration 81, loss = 0.51206446
Iteration 82, loss = 0.51177133
Iteration 83, loss = 0.51154174
Iteration 84, loss = 0.51136149
Iteration 85, loss = 0.51113468
Iteration 86, loss = 0.51094532
Iteration 87, loss = 0.51076892
Iteration 88, loss = 0.51054961
Iteration 89, loss = 0.51047071
Iteration 90, loss = 0.51017552
Iteration 91, loss = 0.50988247
Iteration 92, loss = 0.50964731
Iteration 93, loss = 0.50950506
Iteration 94, loss = 0.50942245
Iteration 95, loss = 0.50927693
Iteration 96, loss = 0.50904713
Iteration 97, loss = 0.50886644
Iteration 98, loss = 0.50874317
Iteration 99, loss = 0.50851651
Iteration 100, loss = 0.50826290
Iteration 101, loss = 0.50822038
Iteration 102, loss = 0.50803871
Iteration 103, loss = 0.50773456
Iteration 104, loss = 0.50769306
Iteration 105, loss = 0.50746090
Iteration 106, loss = 0.50734237
Iteration 107, loss = 0.50707169
Iteration 108, loss = 0.50692284
Iteration 109, loss = 0.50678216
Iteration 110, loss = 0.50672816
Iteration 111, loss = 0.50669579
Iteration 112, loss = 0.50631628
Iteration 113, loss = 0.50611043
Iteration 114, loss = 0.50609568
Iteration 115, loss = 0.50580458
Iteration 116, loss = 0.50563608
Iteration 117, loss = 0.50563289
Iteration 118, loss = 0.50536645
Iteration 119, loss = 0.50506414
Iteration 120, loss = 0.50502464
Iteration 121, loss = 0.50490143
Iteration 122, loss = 0.50476650
Iteration 123, loss = 0.50472302
Iteration 124, loss = 0.50454752
Iteration 125, loss = 0.50426607
Iteration 126, loss = 0.50428496
Iteration 127, loss = 0.50417823
Iteration 128, loss = 0.50395267
Iteration 129, loss = 0.50387063
Iteration 130, loss = 0.50374513
Iteration 131, loss = 0.50358502
Iteration 132, loss = 0.50343306
Iteration 133, loss = 0.50326027
Iteration 134, loss = 0.50316628
Iteration 135, loss = 0.50322704
Iteration 136, loss = 0.50310326
Iteration 137, loss = 0.50283997
Iteration 138, loss = 0.50286512
Iteration 139, loss = 0.50266631
Iteration 140, loss = 0.50257749
Iteration 141, loss = 0.50246944
Iteration 142, loss = 0.50247814
Iteration 143, loss = 0.50231405
Iteration 144, loss = 0.50210835
Iteration 145, loss = 0.50187892
Iteration 146, loss = 0.50194973
Iteration 147, loss = 0.50192853
Iteration 148, loss = 0.50171110
Iteration 149, loss = 0.50167685
Iteration 150, loss = 0.50142404
Iteration 151, loss = 0.50143857
Iteration 152, loss = 0.50140535
Iteration 153, loss = 0.50128012
Iteration 154, loss = 0.50114698
Iteration 155, loss = 0.50105312
Iteration 156, loss = 0.50091475
Iteration 157, loss = 0.50097190
Iteration 158, loss = 0.50073863
Iteration 159, loss = 0.50069505
Iteration 160, loss = 0.50062139
Iteration 161, loss = 0.50057833
Iteration 162, loss = 0.50047397
Iteration 163, loss = 0.50031148
Iteration 164, loss = 0.50019555
Iteration 165, loss = 0.50001203
Iteration 166, loss = 0.50004413
Iteration 167, loss = 0.49999146
Iteration 168, loss = 0.49979514
Iteration 169, loss = 0.49971163
Iteration 170, loss = 0.49968357
Iteration 171, loss = 0.49963152
Iteration 172, loss = 0.49953938
Iteration 173, loss = 0.49955573
Iteration 174, loss = 0.49928244
Iteration 175, loss = 0.49927045
Iteration 176, loss = 0.49928688
Iteration 177, loss = 0.49919072
Iteration 178, loss = 0.49911493
Iteration 179, loss = 0.49905720
Iteration 180, loss = 0.49903983
Iteration 181, loss = 0.49903576
Iteration 182, loss = 0.49897410
Iteration 183, loss = 0.49884214
Iteration 184, loss = 0.49878745
Iteration 185, loss = 0.49871309
Iteration 186, loss = 0.49867811
Iteration 187, loss = 0.49870746
Iteration 188, loss = 0.49853834
Iteration 189, loss = 0.49851798
Iteration 190, loss = 0.49853780
Iteration 191, loss = 0.49852715
Iteration 192, loss = 0.49833788
Iteration 193, loss = 0.49814877
Iteration 194, loss = 0.49829078
Iteration 195, loss = 0.49832742
Iteration 196, loss = 0.49812172
Iteration 197, loss = 0.49811028
Iteration 198, loss = 0.49808138
Iteration 199, loss = 0.49801679
Iteration 200, loss = 0.49788048
Accuracy: 0.6670442620863464
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 151587
False Positives (FP): 74044
False Negatives (FN): 1789
True Positives (TP): 337
######ROC-AUC PLOT DONE######
Precision: 0.004530726932953308
Recall: 0.15851364063969897
F1-Score: 0.008809651404446653
ROC AUC: 0.41517475712817814
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 154176
False Positives (FP): 71455
False Negatives (FN): 1811
True Positives (TP): 315
Precision: 0.0043890204820955835
Recall: 0.14816556914393228
F1-Score: 0.008525495290678793
ROC AUC: 0.41573796493282084


with hierarchicall and seperated collab calc:
######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
######UPSAMPLING DONE######
Value counts: {0.0: 673437, 1.0: 673437}
Iteration 1, loss = 0.59331394
Iteration 2, loss = 0.56707976
Iteration 3, loss = 0.55775477
Iteration 4, loss = 0.55309405
Iteration 5, loss = 0.55004646
Iteration 6, loss = 0.54786175
Iteration 7, loss = 0.54582629
Iteration 8, loss = 0.54412536
Iteration 9, loss = 0.54230382
Iteration 10, loss = 0.54077229
Iteration 11, loss = 0.53928148
Iteration 12, loss = 0.53780741
Iteration 13, loss = 0.53647660
Iteration 14, loss = 0.53521406
Iteration 15, loss = 0.53398543
Iteration 16, loss = 0.53280554
Iteration 17, loss = 0.53153338
Iteration 18, loss = 0.53027842
Iteration 19, loss = 0.52910441
Iteration 20, loss = 0.52830040
Iteration 21, loss = 0.52729543
Iteration 22, loss = 0.52618558
Iteration 23, loss = 0.52526208
Iteration 24, loss = 0.52450106
Iteration 25, loss = 0.52364504
Iteration 26, loss = 0.52285896
Iteration 27, loss = 0.52210294
Iteration 28, loss = 0.52141280
Iteration 29, loss = 0.52081439
Iteration 30, loss = 0.52014632
Iteration 31, loss = 0.51956236
Iteration 32, loss = 0.51873401
Iteration 33, loss = 0.51761603
Iteration 34, loss = 0.51700052
Iteration 35, loss = 0.51640373
Iteration 36, loss = 0.51581096
Iteration 37, loss = 0.51541512
Iteration 38, loss = 0.51490282
Iteration 39, loss = 0.51446336
Iteration 40, loss = 0.51397274
Iteration 41, loss = 0.51360990
Iteration 42, loss = 0.51317629
Iteration 43, loss = 0.51288312
Iteration 44, loss = 0.51249955
Iteration 45, loss = 0.51210277
Iteration 46, loss = 0.51163759
Iteration 47, loss = 0.51137020
Iteration 48, loss = 0.51112375
Iteration 49, loss = 0.51078685
Iteration 50, loss = 0.51049510
Iteration 51, loss = 0.51017028
Iteration 52, loss = 0.50979892
Iteration 53, loss = 0.50962003
Iteration 54, loss = 0.50940055
Iteration 55, loss = 0.50909836
Iteration 56, loss = 0.50875847
Iteration 57, loss = 0.50862593
Iteration 58, loss = 0.50847500
Iteration 59, loss = 0.50815347
Iteration 60, loss = 0.50793696
Iteration 61, loss = 0.50780023
Iteration 62, loss = 0.50754326
Iteration 63, loss = 0.50724597
Iteration 64, loss = 0.50708778
Iteration 65, loss = 0.50704896
Iteration 66, loss = 0.50680935
Iteration 67, loss = 0.50666400
Iteration 68, loss = 0.50646652
Iteration 69, loss = 0.50620113
Iteration 70, loss = 0.50604829
Iteration 71, loss = 0.50589669
Iteration 72, loss = 0.50559575
Iteration 73, loss = 0.50540674
Iteration 74, loss = 0.50530080
Iteration 75, loss = 0.50520506
Iteration 76, loss = 0.50506778
Iteration 77, loss = 0.50481280
Iteration 78, loss = 0.50471228
Iteration 79, loss = 0.50462034
Iteration 80, loss = 0.50435311
Iteration 81, loss = 0.50423783
Iteration 82, loss = 0.50409441
Iteration 83, loss = 0.50381075
Iteration 84, loss = 0.50371934
Iteration 85, loss = 0.50344801
Iteration 86, loss = 0.50344082
Iteration 87, loss = 0.50324756
Iteration 88, loss = 0.50299877
Iteration 89, loss = 0.50289929
Iteration 90, loss = 0.50276078
Iteration 91, loss = 0.50226520
Iteration 92, loss = 0.50193753
Iteration 93, loss = 0.50144863
Iteration 94, loss = 0.50096120
Iteration 95, loss = 0.50069845
Iteration 96, loss = 0.50039318
Iteration 97, loss = 0.50000256
Iteration 98, loss = 0.49973256
Iteration 99, loss = 0.49935768
Iteration 100, loss = 0.49914454
Iteration 101, loss = 0.49887888
Iteration 102, loss = 0.49872024
Iteration 103, loss = 0.49851738
Iteration 104, loss = 0.49849642
Iteration 105, loss = 0.49807077
Iteration 106, loss = 0.49782851
Iteration 107, loss = 0.49764722
Iteration 108, loss = 0.49748186
Iteration 109, loss = 0.49715472
Iteration 110, loss = 0.49704689
Iteration 111, loss = 0.49681273
Iteration 112, loss = 0.49664402
Iteration 113, loss = 0.49651918
Iteration 114, loss = 0.49638520
Iteration 115, loss = 0.49618491
Iteration 116, loss = 0.49594112
Iteration 117, loss = 0.49585812
Iteration 118, loss = 0.49583947
Iteration 119, loss = 0.49553747
Iteration 120, loss = 0.49539252
Iteration 121, loss = 0.49549124
Iteration 122, loss = 0.49535054
Iteration 123, loss = 0.49512556
Iteration 124, loss = 0.49499698
Iteration 125, loss = 0.49483485
Iteration 126, loss = 0.49473080
Iteration 127, loss = 0.49470252
Iteration 128, loss = 0.49444688
Iteration 129, loss = 0.49431243
Iteration 130, loss = 0.49421441
Iteration 131, loss = 0.49413262
Iteration 132, loss = 0.49402265
Iteration 133, loss = 0.49391162
Iteration 134, loss = 0.49369846
Iteration 135, loss = 0.49357521
Iteration 136, loss = 0.49359282
Iteration 137, loss = 0.49340732
Iteration 138, loss = 0.49323201
Iteration 139, loss = 0.49311142
Iteration 140, loss = 0.49296612
Iteration 141, loss = 0.49297233
Iteration 142, loss = 0.49291840
Iteration 143, loss = 0.49274538
Iteration 144, loss = 0.49264296
Iteration 145, loss = 0.49259565
Iteration 146, loss = 0.49244167
Iteration 147, loss = 0.49235043
Iteration 148, loss = 0.49228222
Iteration 149, loss = 0.49211631
Iteration 150, loss = 0.49200863
Iteration 151, loss = 0.49203170
Iteration 152, loss = 0.49191802
Iteration 153, loss = 0.49176307
Iteration 154, loss = 0.49180600
Iteration 155, loss = 0.49166260
Iteration 156, loss = 0.49152039
Iteration 157, loss = 0.49138607
Iteration 158, loss = 0.49131513
Iteration 159, loss = 0.49134293
Iteration 160, loss = 0.49125612
Iteration 161, loss = 0.49120994
Iteration 162, loss = 0.49105532
Iteration 163, loss = 0.49095007
Iteration 164, loss = 0.49102933
Iteration 165, loss = 0.49094323
Iteration 166, loss = 0.49086737
Iteration 167, loss = 0.49072172
Iteration 168, loss = 0.49077948
Iteration 169, loss = 0.49068949
Iteration 170, loss = 0.49067429
Iteration 171, loss = 0.49059107
Iteration 172, loss = 0.49042552
Iteration 173, loss = 0.49056900
Iteration 174, loss = 0.49045461
Iteration 175, loss = 0.49016454
Iteration 176, loss = 0.49035190
Iteration 177, loss = 0.49031764
Iteration 178, loss = 0.49024266
Iteration 179, loss = 0.49015084
Iteration 180, loss = 0.49005557
Iteration 181, loss = 0.48999926
Iteration 182, loss = 0.49003494
Iteration 183, loss = 0.48994327
Iteration 184, loss = 0.48974434
Iteration 185, loss = 0.48980083
Iteration 186, loss = 0.48971489
Iteration 187, loss = 0.48966913
Iteration 188, loss = 0.48955985
Iteration 189, loss = 0.48954802
Iteration 190, loss = 0.48952231
Iteration 191, loss = 0.48931445
Iteration 192, loss = 0.48932438
Iteration 193, loss = 0.48929265
Iteration 194, loss = 0.48928400
Iteration 195, loss = 0.48936542
Iteration 196, loss = 0.48919261
Iteration 197, loss = 0.48911993
Iteration 198, loss = 0.48908164
Iteration 199, loss = 0.48901896
Iteration 200, loss = 0.48905459
/home/ladan102/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  ConvergenceWarning,
Accuracy: 0.633253862669424
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 143795
False Positives (FP): 81836
False Negatives (FN): 1693
True Positives (TP): 433
######ROC-AUC PLOT DONE######
Precision: 0.005263221869720064
Recall: 0.20366886171213547
F1-Score: 0.01026127140233426
ROC AUC: 0.4204852368180145
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 144518
False Positives (FP): 81113
False Negatives (FN): 1718
True Positives (TP): 408
Precision: 0.005004845377264754
Recall: 0.19190968955785512
F1-Score: 0.009755281121857329
ROC AUC: 0.41620782420108143

with old dataset no ordering...         however the result is biased due to the united clustering and network creation
######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
######UPSAMPLING DONE######
Value counts: {0.0: 674301, 1.0: 674301}
Iteration 1, loss = 0.60448875
Iteration 2, loss = 0.57763416
Iteration 3, loss = 0.56453606
Iteration 4, loss = 0.55594233
Iteration 5, loss = 0.54761529
Iteration 6, loss = 0.53889518
Iteration 7, loss = 0.53022712
Iteration 8, loss = 0.52236395
Iteration 9, loss = 0.51524964
Iteration 10, loss = 0.50764558
Iteration 11, loss = 0.50072594
Iteration 12, loss = 0.49533357
Iteration 13, loss = 0.49036305
Iteration 14, loss = 0.48645239
Iteration 15, loss = 0.48317325
Iteration 16, loss = 0.47972830
Iteration 17, loss = 0.47675487
Iteration 18, loss = 0.47428093
Iteration 19, loss = 0.47170559
Iteration 20, loss = 0.46963191
Iteration 21, loss = 0.46739206
Iteration 22, loss = 0.46565721
Iteration 23, loss = 0.46418959
Iteration 24, loss = 0.46243927
Iteration 25, loss = 0.46129074
Iteration 26, loss = 0.45993311
Iteration 27, loss = 0.45874962
Iteration 28, loss = 0.45777213
Iteration 29, loss = 0.45691558
Iteration 30, loss = 0.45591055
Iteration 31, loss = 0.45522193
Iteration 32, loss = 0.45432234
Iteration 33, loss = 0.45341885
Iteration 34, loss = 0.45255944
Iteration 35, loss = 0.45186966
Iteration 36, loss = 0.45123723
Iteration 37, loss = 0.45022146
Iteration 38, loss = 0.44936801
Iteration 39, loss = 0.44873695
Iteration 40, loss = 0.44802189
Iteration 41, loss = 0.44744171
Iteration 42, loss = 0.44682153
Iteration 43, loss = 0.44624362
Iteration 44, loss = 0.44537702
Iteration 45, loss = 0.44484608
Iteration 46, loss = 0.44396251
Iteration 47, loss = 0.44326037
Iteration 48, loss = 0.44231142
Iteration 49, loss = 0.44188605
Iteration 50, loss = 0.44101109
Iteration 51, loss = 0.44006223
Iteration 52, loss = 0.43948664
Iteration 53, loss = 0.43860221
Iteration 54, loss = 0.43782734
Iteration 55, loss = 0.43702020
Iteration 56, loss = 0.43631763
Iteration 57, loss = 0.43566983
Iteration 58, loss = 0.43509862
Iteration 59, loss = 0.43438086
Iteration 60, loss = 0.43370042
Iteration 61, loss = 0.43307357
Iteration 62, loss = 0.43246148
Iteration 63, loss = 0.43171888
Iteration 64, loss = 0.43120077
Iteration 65, loss = 0.43063348
Iteration 66, loss = 0.43019350
Iteration 67, loss = 0.42952084
Iteration 68, loss = 0.42895047
Iteration 69, loss = 0.42823457
Iteration 70, loss = 0.42777562
Iteration 71, loss = 0.42766435
Iteration 72, loss = 0.42689245
Iteration 73, loss = 0.42637250
Iteration 74, loss = 0.42599660
Iteration 75, loss = 0.42550694
Iteration 76, loss = 0.42499361
Iteration 77, loss = 0.42448242
Iteration 78, loss = 0.42386674
Iteration 79, loss = 0.42364742
Iteration 80, loss = 0.42296717
Iteration 81, loss = 0.42277193
Iteration 82, loss = 0.42230014
Iteration 83, loss = 0.42180627
Iteration 84, loss = 0.42127644
Iteration 85, loss = 0.42091801
Iteration 86, loss = 0.42022798
Iteration 87, loss = 0.41970671
Iteration 88, loss = 0.41943060
Iteration 89, loss = 0.41896227
Iteration 90, loss = 0.41876011
Iteration 91, loss = 0.41816866
Iteration 92, loss = 0.41818391
Iteration 93, loss = 0.41753202
Iteration 94, loss = 0.41736069
Iteration 95, loss = 0.41698382
Iteration 96, loss = 0.41654312
Iteration 97, loss = 0.41616654
Iteration 98, loss = 0.41593659
Iteration 99, loss = 0.41545995
Iteration 100, loss = 0.41527068
Iteration 101, loss = 0.41522223
Iteration 102, loss = 0.41458755
Iteration 103, loss = 0.41401778
Iteration 104, loss = 0.41401666
Iteration 105, loss = 0.41342072
Iteration 106, loss = 0.41371906
Iteration 107, loss = 0.41297818
Iteration 108, loss = 0.41254524
Iteration 109, loss = 0.41259735
Iteration 110, loss = 0.41199097
Iteration 111, loss = 0.41178906
Iteration 112, loss = 0.41163410
Iteration 113, loss = 0.41130815
Iteration 114, loss = 0.41113393
Iteration 115, loss = 0.41075872
Iteration 116, loss = 0.41093794
Iteration 117, loss = 0.41048597
Iteration 118, loss = 0.41017251
Iteration 119, loss = 0.40994897
Iteration 120, loss = 0.40966871
Iteration 121, loss = 0.40926461
Iteration 122, loss = 0.40925755
Iteration 123, loss = 0.40899282
Iteration 124, loss = 0.40872187
Iteration 125, loss = 0.40844931
Iteration 126, loss = 0.40842283
Iteration 127, loss = 0.40830039
Iteration 128, loss = 0.40783400
Iteration 129, loss = 0.40782953
Iteration 130, loss = 0.40766357
Iteration 131, loss = 0.40742450
Iteration 132, loss = 0.40731393
Iteration 133, loss = 0.40700839
Iteration 134, loss = 0.40684726
Iteration 135, loss = 0.40674438
Iteration 136, loss = 0.40635390
Iteration 137, loss = 0.40650331
Iteration 138, loss = 0.40613576
Iteration 139, loss = 0.40593157
Iteration 140, loss = 0.40561538
Iteration 141, loss = 0.40548701
Iteration 142, loss = 0.40514200
Iteration 143, loss = 0.40483788
Iteration 144, loss = 0.40479504
Iteration 145, loss = 0.40470786
Iteration 146, loss = 0.40452562
Iteration 147, loss = 0.40424760
Iteration 148, loss = 0.40392930
Iteration 149, loss = 0.40381267
Iteration 150, loss = 0.40358333
Iteration 151, loss = 0.40346940
Iteration 152, loss = 0.40325019
Iteration 153, loss = 0.40315796
Iteration 154, loss = 0.40292998
Iteration 155, loss = 0.40277053
Iteration 156, loss = 0.40264499
Iteration 157, loss = 0.40221791
Iteration 158, loss = 0.40197350
Iteration 159, loss = 0.40167902
Iteration 160, loss = 0.40155813
Iteration 161, loss = 0.40143829
Iteration 162, loss = 0.40111308
Iteration 163, loss = 0.40079555
Iteration 164, loss = 0.40073695
Iteration 165, loss = 0.40055817
Iteration 166, loss = 0.40013324
Iteration 167, loss = 0.40004190
Iteration 168, loss = 0.39994266
Iteration 169, loss = 0.39988564
Iteration 170, loss = 0.39931378
Iteration 171, loss = 0.39930413
Iteration 172, loss = 0.39885540
Iteration 173, loss = 0.39887455
Iteration 174, loss = 0.39845681
Iteration 175, loss = 0.39846806
Iteration 176, loss = 0.39820404
Iteration 177, loss = 0.39800707
Iteration 178, loss = 0.39812452
Iteration 179, loss = 0.39774373
Iteration 180, loss = 0.39743915
Iteration 181, loss = 0.39743544
Iteration 182, loss = 0.39712789
Iteration 183, loss = 0.39690384
Iteration 184, loss = 0.39757415
Iteration 185, loss = 0.39713952
Iteration 186, loss = 0.39654379
Iteration 187, loss = 0.39628400
Iteration 188, loss = 0.39608743
Iteration 189, loss = 0.39610614
Iteration 190, loss = 0.39573448
Iteration 191, loss = 0.39574648
Iteration 192, loss = 0.39565030
Iteration 193, loss = 0.39526684
Iteration 194, loss = 0.39523477
Iteration 195, loss = 0.39500890
Iteration 196, loss = 0.39489770
Iteration 197, loss = 0.39454840
Iteration 198, loss = 0.39443191
Iteration 199, loss = 0.39436367
Iteration 200, loss = 0.39408958
/home/ladan102/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  ConvergenceWarning,
Accuracy: 0.9062070540093169
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 204408
False Positives (FP): 20359
False Negatives (FN): 1003
True Positives (TP): 1987
######ROC-AUC PLOT DONE######
Precision: 0.0889197171753334
Recall: 0.6645484949832776
F1-Score: 0.15685191032522894
ROC AUC: 0.786985125867913
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 219925
False Positives (FP): 4842
False Negatives (FN): 1464
True Positives (TP): 1526
Precision: 0.2396356783919598
Recall: 0.5103678929765886
F1-Score: 0.3261380636888224
ROC AUC: 0.7444127923597968
however the result is prone to data leakage due to the united clustering and network creation

with hierarchical and seperated collab calc and collab profile:
######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
######UPSAMPLING DONE######
Value counts: {0.0: 673437, 1.0: 673437}
Iteration 1, loss = 0.59392243
Iteration 2, loss = 0.57245757
Iteration 3, loss = 0.56305142
Iteration 4, loss = 0.55683742
Iteration 5, loss = 0.55292882
Iteration 6, loss = 0.55015478
Iteration 7, loss = 0.54770656
Iteration 8, loss = 0.54557477
Iteration 9, loss = 0.54350024
Iteration 10, loss = 0.54171813
Iteration 11, loss = 0.53972742
Iteration 12, loss = 0.53793805
Iteration 13, loss = 0.53626697
Iteration 14, loss = 0.53465083
Iteration 15, loss = 0.53340494
Iteration 16, loss = 0.53226839
Iteration 17, loss = 0.53116959
Iteration 18, loss = 0.53007597
Iteration 19, loss = 0.52913902
Iteration 20, loss = 0.52806136
Iteration 21, loss = 0.52725128
Iteration 22, loss = 0.52649737
Iteration 23, loss = 0.52566089
Iteration 24, loss = 0.52477288
Iteration 25, loss = 0.52395491
Iteration 26, loss = 0.52308469
Iteration 27, loss = 0.52234807
Iteration 28, loss = 0.52157134
Iteration 29, loss = 0.52103425
Iteration 30, loss = 0.52032162
Iteration 31, loss = 0.51976420
Iteration 32, loss = 0.51921449
Iteration 33, loss = 0.51883194
Iteration 34, loss = 0.51833020
Iteration 35, loss = 0.51773907
Iteration 36, loss = 0.51731600
Iteration 37, loss = 0.51698492
Iteration 38, loss = 0.51663618
Iteration 39, loss = 0.51610627
Iteration 40, loss = 0.51579080
Iteration 41, loss = 0.51540353
Iteration 42, loss = 0.51503968
Iteration 43, loss = 0.51470338
Iteration 44, loss = 0.51440721
Iteration 45, loss = 0.51398464
Iteration 46, loss = 0.51359847
Iteration 47, loss = 0.51320564
Iteration 48, loss = 0.51284576
Iteration 49, loss = 0.51258864
Iteration 50, loss = 0.51221563
Iteration 51, loss = 0.51188436
Iteration 52, loss = 0.51174507
Iteration 53, loss = 0.51155712
Iteration 54, loss = 0.51130992
Iteration 55, loss = 0.51095313
Iteration 56, loss = 0.51075415
Iteration 57, loss = 0.51048120
Iteration 58, loss = 0.51037029
Iteration 59, loss = 0.51010838
Iteration 60, loss = 0.50980736
Iteration 61, loss = 0.50968249
Iteration 62, loss = 0.50945038
Iteration 63, loss = 0.50922604
Iteration 64, loss = 0.50914133
Iteration 65, loss = 0.50887557
Iteration 66, loss = 0.50868956
Iteration 67, loss = 0.50842687
Iteration 68, loss = 0.50834425
Iteration 69, loss = 0.50812070
Iteration 70, loss = 0.50785524
Iteration 71, loss = 0.50778257
Iteration 72, loss = 0.50762862
Iteration 73, loss = 0.50736555
Iteration 74, loss = 0.50718131
Iteration 75, loss = 0.50706559
Iteration 76, loss = 0.50677972
Iteration 77, loss = 0.50678665
Iteration 78, loss = 0.50650010
Iteration 79, loss = 0.50638217
Iteration 80, loss = 0.50624763
Iteration 81, loss = 0.50592705
Iteration 82, loss = 0.50583827
Iteration 83, loss = 0.50584266
Iteration 84, loss = 0.50560917
Iteration 85, loss = 0.50527806
Iteration 86, loss = 0.50521551
Iteration 87, loss = 0.50503158
Iteration 88, loss = 0.50485273
Iteration 89, loss = 0.50480338
Iteration 90, loss = 0.50457373
Iteration 91, loss = 0.50425453
Iteration 92, loss = 0.50431128
Iteration 93, loss = 0.50397274
Iteration 94, loss = 0.50395594
Iteration 95, loss = 0.50376519
Iteration 96, loss = 0.50367700
Iteration 97, loss = 0.50354928
Iteration 98, loss = 0.50336235
Iteration 99, loss = 0.50328742
Iteration 100, loss = 0.50302827
Iteration 101, loss = 0.50300254
Iteration 102, loss = 0.50290408
Iteration 103, loss = 0.50288718
Iteration 104, loss = 0.50261898
Iteration 105, loss = 0.50232106
Iteration 106, loss = 0.50222970
Iteration 107, loss = 0.50233493
Iteration 108, loss = 0.50211558
Iteration 109, loss = 0.50202853
Iteration 110, loss = 0.50189446
Iteration 111, loss = 0.50164375
Iteration 112, loss = 0.50168833
Iteration 113, loss = 0.50147848
Iteration 114, loss = 0.50147119
Iteration 115, loss = 0.50147031
Iteration 116, loss = 0.50097696
Iteration 117, loss = 0.50072940
Iteration 118, loss = 0.50057667
Iteration 119, loss = 0.50026869
Iteration 120, loss = 0.50019041
Iteration 121, loss = 0.50021289
Iteration 122, loss = 0.50003290
Iteration 123, loss = 0.49974138
Iteration 124, loss = 0.49957831
Iteration 125, loss = 0.49954530
Iteration 126, loss = 0.49946355
Iteration 127, loss = 0.49929862
Iteration 128, loss = 0.49928894
Iteration 129, loss = 0.49903986
Iteration 130, loss = 0.49920936
Iteration 131, loss = 0.49887675
Iteration 132, loss = 0.49872771
Iteration 133, loss = 0.49875508
Iteration 134, loss = 0.49852223
Iteration 135, loss = 0.49836950
Iteration 136, loss = 0.49849094
Iteration 137, loss = 0.49832556
Iteration 138, loss = 0.49833464
Iteration 139, loss = 0.49802881
Iteration 140, loss = 0.49802794
Iteration 141, loss = 0.49802472
Iteration 142, loss = 0.49762074
Iteration 143, loss = 0.49755260
Iteration 144, loss = 0.49737591
Iteration 145, loss = 0.49730128
Iteration 146, loss = 0.49719173
Iteration 147, loss = 0.49714521
Iteration 148, loss = 0.49696605
Iteration 149, loss = 0.49697306
Iteration 150, loss = 0.49678212
Iteration 151, loss = 0.49696764
Iteration 152, loss = 0.49678044
Iteration 153, loss = 0.49671586
Iteration 154, loss = 0.49630886
Iteration 155, loss = 0.49646051
Iteration 156, loss = 0.49628846
Iteration 157, loss = 0.49632289
Iteration 158, loss = 0.49628075
Iteration 159, loss = 0.49605125
Iteration 160, loss = 0.49620481
Iteration 161, loss = 0.49598194
Iteration 162, loss = 0.49590019
Iteration 163, loss = 0.49588905
Iteration 164, loss = 0.49570595
Iteration 165, loss = 0.49569082
Iteration 166, loss = 0.49578559
Iteration 167, loss = 0.49544783
Iteration 168, loss = 0.49553850
Iteration 169, loss = 0.49549825
Iteration 170, loss = 0.49538705
Iteration 171, loss = 0.49531710
Iteration 172, loss = 0.49515582
Iteration 173, loss = 0.49499203
Iteration 174, loss = 0.49505955
Iteration 175, loss = 0.49501833
Iteration 176, loss = 0.49495652
Iteration 177, loss = 0.49490145
Iteration 178, loss = 0.49488123
Iteration 179, loss = 0.49482475
Iteration 180, loss = 0.49464949
Iteration 181, loss = 0.49483174
Iteration 182, loss = 0.49475718
Iteration 183, loss = 0.49466053
Iteration 184, loss = 0.49444120
Iteration 185, loss = 0.49455760
Iteration 186, loss = 0.49434792
Iteration 187, loss = 0.49423648
Iteration 188, loss = 0.49422985
Iteration 189, loss = 0.49425581
Iteration 190, loss = 0.49431919
Iteration 191, loss = 0.49412025
Iteration 192, loss = 0.49419591
Iteration 193, loss = 0.49416978
Iteration 194, loss = 0.49403214
Iteration 195, loss = 0.49383932
Iteration 196, loss = 0.49382896
Iteration 197, loss = 0.49369792
Iteration 198, loss = 0.49369266
Iteration 199, loss = 0.49365130
Iteration 200, loss = 0.49364263
/home/ladan102/.local/lib/python3.7/site-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  ConvergenceWarning,
Accuracy: 0.4756868065525977
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 107427
False Positives (FP): 118204
False Negatives (FN): 1212
True Positives (TP): 914
######ROC-AUC PLOT DONE######
Precision: 0.007673063684749576
Recall: 0.4299153339604892
F1-Score: 0.015077034739863417
ROC AUC: 0.45301671028546425
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 130945
False Positives (FP): 94686
False Negatives (FN): 1321
True Positives (TP): 805
Precision: 0.00843011383271722
Recall: 0.3786453433678269
F1-Score: 0.01649302887816671
ROC AUC: 0.4794977806006847

result is better than previous result

with collab feature, with seperated, order of data fixed:
######TRAIN TEST SPLIT DONE######
######UPSAMPLING DONE######
X_train_up type: <class 'numpy.ndarray'>
y_train_up type: <class 'numpy.ndarray'>
X_train_up shape: (1346874, 17)
y_train_up shape: (1346874, 1)
<class 'pandas.core.frame.DataFrame'>
<class 'pandas.core.series.Series'>
Value counts: {0.0: 673437, 1.0: 673437}
        betweenesscentrality_x closnesscentrality_x  ...    profile_y  hit
0                          0.0                  0.0  ...     No Match  0.0
1076361                0.00144             0.217716  ...     No Match  1.0
1076310                0.00144             0.217716  ...     No Match  1.0
1076246                    0.0                  0.0  ...     No Match  1.0
1076194               0.000381             0.186334  ...  1A 2A 3A 4A  1.0

[5 rows x 17 columns]
numerical columns: Index(['betweenesscentrality_x', 'closnesscentrality_x', 'clustering_x',
       'Cluster_x', 'eccentricity_x', 'eigencentrality_x', 'weighted degree_x',
       'betweenesscentrality_y', 'closnesscentrality_y', 'clustering_y',
       'Cluster_y', 'eccentricity_y', 'eigencentrality_y',
       'weighted degree_y'],
      dtype='object')
Index(['profile_x', 'profile_y'], dtype='object')
######PREPROCESSING DONE######
Iteration 1, loss = 0.59383638
Iteration 2, loss = 0.57052851
Iteration 3, loss = 0.56037660
Iteration 4, loss = 0.55431658
Iteration 5, loss = 0.55026102
Iteration 6, loss = 0.54740951
Iteration 7, loss = 0.54521509
Iteration 8, loss = 0.54327537
Iteration 9, loss = 0.54141494
Iteration 10, loss = 0.53986723
Iteration 11, loss = 0.53816754
Iteration 12, loss = 0.53671694
Iteration 13, loss = 0.53535064
Iteration 14, loss = 0.53371708
Iteration 15, loss = 0.53248101
Iteration 16, loss = 0.53147876
Iteration 17, loss = 0.53012597
Iteration 18, loss = 0.52892922
Iteration 19, loss = 0.52786808
Iteration 20, loss = 0.52681989
Iteration 21, loss = 0.52574392
Iteration 22, loss = 0.52498669
Iteration 23, loss = 0.52414641
Iteration 24, loss = 0.52335063
Iteration 25, loss = 0.52267205
Iteration 26, loss = 0.52194917
Iteration 27, loss = 0.52142326
Iteration 28, loss = 0.52070680
Iteration 29, loss = 0.52022128
Iteration 30, loss = 0.51974154
Iteration 31, loss = 0.51914239
Iteration 32, loss = 0.51862986
Iteration 33, loss = 0.51832146
Iteration 34, loss = 0.51786584
Iteration 35, loss = 0.51740816
Iteration 36, loss = 0.51702595
Iteration 37, loss = 0.51662214
Iteration 38, loss = 0.51598911
Iteration 39, loss = 0.51570590
Iteration 40, loss = 0.51518049
Iteration 41, loss = 0.51488197
Iteration 42, loss = 0.51427829
Iteration 43, loss = 0.51388578
Iteration 44, loss = 0.51345234
Iteration 45, loss = 0.51306520
Iteration 46, loss = 0.51273705
Iteration 47, loss = 0.51233309
Iteration 48, loss = 0.51194910
Iteration 49, loss = 0.51151918
Iteration 50, loss = 0.51108980
Iteration 51, loss = 0.51093766
Iteration 52, loss = 0.51054405
Iteration 53, loss = 0.51019230
Iteration 54, loss = 0.50994494
Iteration 55, loss = 0.50961154
Iteration 56, loss = 0.50925963
Iteration 57, loss = 0.50891585
Iteration 58, loss = 0.50875550
Iteration 59, loss = 0.50849742
Iteration 60, loss = 0.50803743
Iteration 61, loss = 0.50793165
Iteration 62, loss = 0.50761112
Iteration 63, loss = 0.50739764
Iteration 64, loss = 0.50708728
Iteration 65, loss = 0.50700113
Iteration 66, loss = 0.50658692
Iteration 67, loss = 0.50640387
Iteration 68, loss = 0.50622345
Iteration 69, loss = 0.50591698
Iteration 70, loss = 0.50560955
Iteration 71, loss = 0.50532570
Iteration 72, loss = 0.50516914
Iteration 73, loss = 0.50496349
Iteration 74, loss = 0.50457077
Iteration 75, loss = 0.50442923
Iteration 76, loss = 0.50425334
Iteration 77, loss = 0.50403185
Iteration 78, loss = 0.50378444
Iteration 79, loss = 0.50359549
Iteration 80, loss = 0.50335145
Iteration 81, loss = 0.50307037
Iteration 82, loss = 0.50281348
Iteration 83, loss = 0.50264415
Iteration 84, loss = 0.50255522
Iteration 85, loss = 0.50219740
Iteration 86, loss = 0.50198300
Iteration 87, loss = 0.50173440
Iteration 88, loss = 0.50164896
Iteration 89, loss = 0.50123023
Iteration 90, loss = 0.50118232
Iteration 91, loss = 0.50095801
Iteration 92, loss = 0.50074672
Iteration 93, loss = 0.50050824
Iteration 94, loss = 0.50039030
Iteration 95, loss = 0.50024773
Iteration 96, loss = 0.50015582
Iteration 97, loss = 0.49985561
Iteration 98, loss = 0.49955422
Iteration 99, loss = 0.49961568
Iteration 100, loss = 0.49942708
Iteration 101, loss = 0.49923849
Iteration 102, loss = 0.49905041
Iteration 103, loss = 0.49879682
Iteration 104, loss = 0.49868123
Iteration 105, loss = 0.49838785
Iteration 106, loss = 0.49839490
Iteration 107, loss = 0.49828469
Iteration 108, loss = 0.49818243
Iteration 109, loss = 0.49787162
Iteration 110, loss = 0.49787025
Iteration 111, loss = 0.49756157
Iteration 112, loss = 0.49758424
Iteration 113, loss = 0.49744130
Iteration 114, loss = 0.49721929
Iteration 115, loss = 0.49716666
Iteration 116, loss = 0.49693382
Iteration 117, loss = 0.49674385
Iteration 118, loss = 0.49681039
Iteration 119, loss = 0.49648639
Iteration 120, loss = 0.49640314
Iteration 121, loss = 0.49639365
Iteration 122, loss = 0.49621511
Iteration 123, loss = 0.49586515
Iteration 124, loss = 0.49596102
Iteration 125, loss = 0.49585323
Iteration 126, loss = 0.49568001
Iteration 127, loss = 0.49555658
Iteration 128, loss = 0.49543495
Iteration 129, loss = 0.49540501
Iteration 130, loss = 0.49505518
Iteration 131, loss = 0.49512705
Iteration 132, loss = 0.49513101
Iteration 133, loss = 0.49478220
Iteration 134, loss = 0.49478984
Iteration 135, loss = 0.49460541
Iteration 136, loss = 0.49467262
Iteration 137, loss = 0.49432073
Iteration 138, loss = 0.49435258
Iteration 139, loss = 0.49442025
Iteration 140, loss = 0.49418434
Iteration 141, loss = 0.49410740
Iteration 142, loss = 0.49410030
Iteration 143, loss = 0.49397474
Iteration 144, loss = 0.49372464
Iteration 145, loss = 0.49380458
Iteration 146, loss = 0.49373131
Iteration 147, loss = 0.49354370
Iteration 148, loss = 0.49349086
Iteration 149, loss = 0.49340796
Iteration 150, loss = 0.49346326
Iteration 151, loss = 0.49327018
Iteration 152, loss = 0.49305664
Iteration 153, loss = 0.49310009
Iteration 154, loss = 0.49278851
Iteration 155, loss = 0.49292578
Iteration 156, loss = 0.49288580
Iteration 157, loss = 0.49269190
Iteration 158, loss = 0.49252365
Iteration 159, loss = 0.49234874
Iteration 160, loss = 0.49236053
Iteration 161, loss = 0.49235383
Iteration 162, loss = 0.49205056
Iteration 163, loss = 0.49206706
Iteration 164, loss = 0.49206351
Iteration 165, loss = 0.49193305
Iteration 166, loss = 0.49184468
Iteration 167, loss = 0.49178560
Iteration 168, loss = 0.49176388
Iteration 169, loss = 0.49144192
Iteration 170, loss = 0.49133527
Iteration 171, loss = 0.49143661
Iteration 172, loss = 0.49127583
Iteration 173, loss = 0.49127295
Iteration 174, loss = 0.49131555
Iteration 175, loss = 0.49118080
Iteration 176, loss = 0.49118513
Iteration 177, loss = 0.49104388
Iteration 178, loss = 0.49089877
Iteration 179, loss = 0.49082848
Iteration 180, loss = 0.49073436
Iteration 181, loss = 0.49068749
Iteration 182, loss = 0.49063933
Iteration 183, loss = 0.49051138
Iteration 184, loss = 0.49049139
Iteration 185, loss = 0.49052516
Iteration 186, loss = 0.49020109
Iteration 187, loss = 0.49011873
Iteration 188, loss = 0.49018174
Iteration 189, loss = 0.49000932
Iteration 190, loss = 0.48998801
Iteration 191, loss = 0.48979223
Iteration 192, loss = 0.48991249
Iteration 193, loss = 0.48978404
Iteration 194, loss = 0.48959431
Iteration 195, loss = 0.48954549
Iteration 196, loss = 0.48951388
Iteration 197, loss = 0.48961313
Iteration 198, loss = 0.48946033
Iteration 199, loss = 0.48928847
Iteration 200, loss = 0.48938290
Accuracy: 0.7071396268830377
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 160751
False Positives (FP): 64880
False Negatives (FN): 1821
True Positives (TP): 305
######ROC-AUC PLOT DONE######
Precision: 0.004678990565314106
Recall: 0.14346190028222014
F1-Score: 0.009062411790049177
ROC AUC: 0.4279563801573756
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 167377
False Positives (FP): 58254
False Negatives (FN): 1865
True Positives (TP): 261
Precision: 0.0044603947705716485
Recall: 0.12276575729068674
F1-Score: 0.00860803746640062
ROC AUC: 0.43229157470217955

good struct:
Accuracy: 0.527601786114148
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 119376
False Positives (FP): 106255
False Negatives (FN): 1337
True Positives (TP): 789
######ROC-AUC PLOT DONE######
Precision: 0.007370800792197601
Recall: 0.3711194731890875
F1-Score: 0.014454520472657323
ROC AUC: 0.45014137794635034
######ROC-AUC PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 121175
False Positives (FP): 104456
False Negatives (FN): 1384
True Positives (TP): 742
Precision: 0.007053366033574783
Recall: 0.34901222953904043
F1-Score: 0.013827289329506916
ROC AUC: 0.4430308299017494