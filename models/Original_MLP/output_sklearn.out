######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
class count:  [674301   8969]
######UPSAMPLING DONE######
Iteration 1, loss = 0.31479191
Iteration 2, loss = 0.26282247
Iteration 3, loss = 0.24052802
Iteration 4, loss = 0.22670966
Iteration 5, loss = 0.21709163
Iteration 6, loss = 0.20889639
Iteration 7, loss = 0.20184493
Iteration 8, loss = 0.19573026
Iteration 9, loss = 0.19047998
Iteration 10, loss = 0.18628230
Iteration 11, loss = 0.18308829
Iteration 12, loss = 0.17978047
Iteration 13, loss = 0.17721888
Iteration 14, loss = 0.17516840
Iteration 15, loss = 0.17315106
Iteration 16, loss = 0.17142500
Iteration 17, loss = 0.16965120
Iteration 18, loss = 0.16819809
Iteration 19, loss = 0.16659810
Iteration 20, loss = 0.16491791
Iteration 21, loss = 0.16360282
Iteration 22, loss = 0.16225859
Iteration 23, loss = 0.16096210
Iteration 24, loss = 0.15977133
Iteration 25, loss = 0.15895313
Iteration 26, loss = 0.15797126
Iteration 27, loss = 0.15684428
Iteration 28, loss = 0.15645218
Iteration 29, loss = 0.15556792
Iteration 30, loss = 0.15473754
Iteration 31, loss = 0.15425672
Iteration 32, loss = 0.15367194
Iteration 33, loss = 0.15296783
Iteration 34, loss = 0.15260522
Iteration 35, loss = 0.15198211
Iteration 36, loss = 0.15150501
Iteration 37, loss = 0.15106033
Iteration 38, loss = 0.15067220
Iteration 39, loss = 0.15006281
Iteration 40, loss = 0.14976495
Iteration 41, loss = 0.14937115
Iteration 42, loss = 0.14913973
Iteration 43, loss = 0.14856178
Iteration 44, loss = 0.14833622
Iteration 45, loss = 0.14780185
Iteration 46, loss = 0.14776911
Iteration 47, loss = 0.14726012
Iteration 48, loss = 0.14678661
Iteration 49, loss = 0.14638775
Iteration 50, loss = 0.14588581
Iteration 51, loss = 0.14559343
Iteration 52, loss = 0.14575459
Iteration 53, loss = 0.14503940
Iteration 54, loss = 0.14479868
Iteration 55, loss = 0.14446535
Iteration 56, loss = 0.14420123
Iteration 57, loss = 0.14365611
Iteration 58, loss = 0.14377801
Iteration 59, loss = 0.14322361
Iteration 60, loss = 0.14316095
Iteration 61, loss = 0.14253153
Iteration 62, loss = 0.14257416
Iteration 63, loss = 0.14229641
Iteration 64, loss = 0.14230303
Iteration 65, loss = 0.14190371
Iteration 66, loss = 0.14156217
Iteration 67, loss = 0.14125269
Iteration 68, loss = 0.14127283
Iteration 69, loss = 0.14108276
Iteration 70, loss = 0.14060111
Iteration 71, loss = 0.14060579
Iteration 72, loss = 0.14060282
Iteration 73, loss = 0.14032758
Iteration 74, loss = 0.14015123
Iteration 75, loss = 0.13971610
Iteration 76, loss = 0.13977521
Iteration 77, loss = 0.13952772
Iteration 78, loss = 0.13949878
Iteration 79, loss = 0.13920839
Iteration 80, loss = 0.13923143
Iteration 81, loss = 0.13866391
Iteration 82, loss = 0.13882117
Iteration 83, loss = 0.13833328
Iteration 84, loss = 0.13821125
Iteration 85, loss = 0.13791625
Iteration 86, loss = 0.13781925
Iteration 87, loss = 0.13769758
Iteration 88, loss = 0.13756248
Iteration 89, loss = 0.13742887
Iteration 90, loss = 0.13727024
Iteration 91, loss = 0.13708869
Iteration 92, loss = 0.13689109
Iteration 93, loss = 0.13671193
Iteration 94, loss = 0.13700002
Iteration 95, loss = 0.13641029
Iteration 96, loss = 0.13630651
Iteration 97, loss = 0.13633456
Iteration 98, loss = 0.13598766
Iteration 99, loss = 0.13605434
Iteration 100, loss = 0.13557471
Iteration 101, loss = 0.13563598
Iteration 102, loss = 0.13550179
Iteration 103, loss = 0.13528800
Iteration 104, loss = 0.13521340
Iteration 105, loss = 0.13492800
Iteration 106, loss = 0.13477263
Iteration 107, loss = 0.13498372
Iteration 108, loss = 0.13457708
Iteration 109, loss = 0.13458480
Iteration 110, loss = 0.13455574
Iteration 111, loss = 0.13414365
Iteration 112, loss = 0.13425217
Iteration 113, loss = 0.13414261
Iteration 114, loss = 0.13427278
Iteration 115, loss = 0.13382869
Iteration 116, loss = 0.13356935
Iteration 117, loss = 0.13373516
Iteration 118, loss = 0.13367202
Iteration 119, loss = 0.13330767
Iteration 120, loss = 0.13330504
Iteration 121, loss = 0.13292584
Iteration 122, loss = 0.13283894
Iteration 123, loss = 0.13275209
Iteration 124, loss = 0.13268236
Iteration 125, loss = 0.13252635
Iteration 126, loss = 0.13245838
Iteration 127, loss = 0.13217807
Iteration 128, loss = 0.13206655
Iteration 129, loss = 0.13202391
Iteration 130, loss = 0.13219216
Iteration 131, loss = 0.13187439
Iteration 132, loss = 0.13184063
Iteration 133, loss = 0.13166052
Iteration 134, loss = 0.13148228
Iteration 135, loss = 0.13128403
Iteration 136, loss = 0.13156894
Iteration 137, loss = 0.13153062
Iteration 138, loss = 0.13105210
Iteration 139, loss = 0.13103702
Iteration 140, loss = 0.13097824
Iteration 141, loss = 0.13079715
Iteration 142, loss = 0.13065422
Iteration 143, loss = 0.13091701
Iteration 144, loss = 0.13072587
Iteration 145, loss = 0.13067462
Iteration 146, loss = 0.13058645
Iteration 147, loss = 0.13042304
Iteration 148, loss = 0.13037318
Iteration 149, loss = 0.13004632
Iteration 150, loss = 0.13013805
Iteration 151, loss = 0.13023646
Iteration 152, loss = 0.13013305
Iteration 153, loss = 0.12992238
Iteration 154, loss = 0.12989276
Iteration 155, loss = 0.12994214
Iteration 156, loss = 0.12980273
Iteration 157, loss = 0.12952438
Iteration 158, loss = 0.12958788
Iteration 159, loss = 0.12930331
Iteration 160, loss = 0.12930482
Iteration 161, loss = 0.12928048
Iteration 162, loss = 0.12907408
Iteration 163, loss = 0.12901140
Iteration 164, loss = 0.12896943
Iteration 165, loss = 0.12892521
Iteration 166, loss = 0.12876197
Iteration 167, loss = 0.12871694
Iteration 168, loss = 0.12846751
Iteration 169, loss = 0.12859738
Iteration 170, loss = 0.12856665
Iteration 171, loss = 0.12832443
Iteration 172, loss = 0.12828018
Iteration 173, loss = 0.12815529
Iteration 174, loss = 0.12812455
Iteration 175, loss = 0.12824066
Iteration 176, loss = 0.12796484
Iteration 177, loss = 0.12781329
Iteration 178, loss = 0.12792161
Iteration 179, loss = 0.12784224
Iteration 180, loss = 0.12779156
Iteration 181, loss = 0.12748979
Iteration 182, loss = 0.12763519
Iteration 183, loss = 0.12737626
Iteration 184, loss = 0.12726372
Iteration 185, loss = 0.12727668
Iteration 186, loss = 0.12721878
Iteration 187, loss = 0.12726701
Iteration 188, loss = 0.12728205
Iteration 189, loss = 0.12700004
Iteration 190, loss = 0.12699277
Iteration 191, loss = 0.12713924
Iteration 192, loss = 0.12690139
Iteration 193, loss = 0.12693125
Iteration 194, loss = 0.12668524
Iteration 195, loss = 0.12679454
Iteration 196, loss = 0.12678343
Iteration 197, loss = 0.12680696
Iteration 198, loss = 0.12639233
Iteration 199, loss = 0.12668935
Iteration 200, loss = 0.12638174
Accuracy: 0.9564140728934786
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 215282
False Positives (FP): 9485
False Negatives (FN): 442
True Positives (TP): 2548
Precision: 0.21175101803374055
Recall: 0.8521739130434782
F1-Score: 0.33921320641682756
ROC AUC: 0.9049873289073651
######ROC-AUC PLOT DONE######
