######PREPROCESSING DONE######
######TRAIN TEST SPLIT DONE######
class count:  [674301   8969]
######UPSAMPLING DONE######
Iteration 1, loss = 0.31785688
Iteration 2, loss = 0.26852857
Iteration 3, loss = 0.24846151
Iteration 4, loss = 0.23495310
Iteration 5, loss = 0.22514866
Iteration 6, loss = 0.21569855
Iteration 7, loss = 0.20671618
Iteration 8, loss = 0.20025478
Iteration 9, loss = 0.19500218
Iteration 10, loss = 0.19029220
Iteration 11, loss = 0.18624844
Iteration 12, loss = 0.18287112
Iteration 13, loss = 0.17958232
Iteration 14, loss = 0.17675833
Iteration 15, loss = 0.17450100
Iteration 16, loss = 0.17231642
Iteration 17, loss = 0.17031015
Iteration 18, loss = 0.16830367
Iteration 19, loss = 0.16682451
Iteration 20, loss = 0.16560072
Iteration 21, loss = 0.16395618
Iteration 22, loss = 0.16209915
Iteration 23, loss = 0.16088891
Iteration 24, loss = 0.15971787
Iteration 25, loss = 0.15861622
Iteration 26, loss = 0.15747907
Iteration 27, loss = 0.15666997
Iteration 28, loss = 0.15539869
Iteration 29, loss = 0.15473323
Iteration 30, loss = 0.15363957
Iteration 31, loss = 0.15270029
Iteration 32, loss = 0.15234452
Iteration 33, loss = 0.15155057
Iteration 34, loss = 0.15091053
Iteration 35, loss = 0.15014404
Iteration 36, loss = 0.14947289
Iteration 37, loss = 0.14886540
Iteration 38, loss = 0.14833739
Iteration 39, loss = 0.14771995
Iteration 40, loss = 0.14699917
Iteration 41, loss = 0.14673192
Iteration 42, loss = 0.14644188
Iteration 43, loss = 0.14574728
Iteration 44, loss = 0.14521866
Iteration 45, loss = 0.14463838
Iteration 46, loss = 0.14450571
Iteration 47, loss = 0.14392532
Iteration 48, loss = 0.14392357
Iteration 49, loss = 0.14333758
Iteration 50, loss = 0.14300188
Iteration 51, loss = 0.14264529
Iteration 52, loss = 0.14226952
Iteration 53, loss = 0.14205345
Iteration 54, loss = 0.14193831
Iteration 55, loss = 0.14167712
Iteration 56, loss = 0.14111580
Iteration 57, loss = 0.14093899
Iteration 58, loss = 0.14087984
Iteration 59, loss = 0.14062442
Iteration 60, loss = 0.14041028
Iteration 61, loss = 0.14019416
Iteration 62, loss = 0.14027695
Iteration 63, loss = 0.13989701
Iteration 64, loss = 0.13985951
Iteration 65, loss = 0.13951251
Iteration 66, loss = 0.13936449
Iteration 67, loss = 0.13915856
Iteration 68, loss = 0.13898210
Iteration 69, loss = 0.13904112
Iteration 70, loss = 0.13858566
Iteration 71, loss = 0.13855352
Iteration 72, loss = 0.13840135
Iteration 73, loss = 0.13864830
Iteration 74, loss = 0.13799410
Iteration 75, loss = 0.13779315
Iteration 76, loss = 0.13773085
Iteration 77, loss = 0.13769049
Iteration 78, loss = 0.13740737
Iteration 79, loss = 0.13708967
Iteration 80, loss = 0.13727586
Iteration 81, loss = 0.13691890
Iteration 82, loss = 0.13672637
Iteration 83, loss = 0.13687176
Iteration 84, loss = 0.13658010
Iteration 85, loss = 0.13658387
Iteration 86, loss = 0.13658085
Iteration 87, loss = 0.13613080
Iteration 88, loss = 0.13652036
Iteration 89, loss = 0.13627575
Iteration 90, loss = 0.13602561
Iteration 91, loss = 0.13609721
Iteration 92, loss = 0.13579552
Iteration 93, loss = 0.13561880
Iteration 94, loss = 0.13563212
Iteration 95, loss = 0.13554958
Iteration 96, loss = 0.13540879
Iteration 97, loss = 0.13526000
Iteration 98, loss = 0.13524456
Iteration 99, loss = 0.13515497
Iteration 100, loss = 0.13507456
Iteration 101, loss = 0.13501899
Iteration 102, loss = 0.13481274
Iteration 103, loss = 0.13457478
Iteration 104, loss = 0.13462953
Iteration 105, loss = 0.13455804
Iteration 106, loss = 0.13449877
Iteration 107, loss = 0.13455796
Iteration 108, loss = 0.13432978
Iteration 109, loss = 0.13442616
Iteration 110, loss = 0.13440902
Iteration 111, loss = 0.13407834
Iteration 112, loss = 0.13406628
Iteration 113, loss = 0.13373131
Iteration 114, loss = 0.13395592
Iteration 115, loss = 0.13385252
Iteration 116, loss = 0.13364252
Iteration 117, loss = 0.13366585
Iteration 118, loss = 0.13357640
Iteration 119, loss = 0.13348482
Iteration 120, loss = 0.13344343
Iteration 121, loss = 0.13322615
Iteration 122, loss = 0.13332176
Iteration 123, loss = 0.13302824
Iteration 124, loss = 0.13288478
Iteration 125, loss = 0.13314117
Iteration 126, loss = 0.13297537
Iteration 127, loss = 0.13279219
Iteration 128, loss = 0.13292487
Iteration 129, loss = 0.13285829
Iteration 130, loss = 0.13274986
Iteration 131, loss = 0.13249763
Iteration 132, loss = 0.13264701
Iteration 133, loss = 0.13245019
Iteration 134, loss = 0.13242525
Iteration 135, loss = 0.13245155
Iteration 136, loss = 0.13226655
Iteration 137, loss = 0.13208331
Iteration 138, loss = 0.13205399
Iteration 139, loss = 0.13207722
Iteration 140, loss = 0.13179404
Iteration 141, loss = 0.13192047
Iteration 142, loss = 0.13171935
Iteration 143, loss = 0.13153875
Iteration 144, loss = 0.13164958
Iteration 145, loss = 0.13133810
Iteration 146, loss = 0.13163311
Iteration 147, loss = 0.13139454
Iteration 148, loss = 0.13130270
Iteration 149, loss = 0.13135904
Iteration 150, loss = 0.13127923
Iteration 151, loss = 0.13120262
Iteration 152, loss = 0.13098313
Iteration 153, loss = 0.13107990
Iteration 154, loss = 0.13112308
Iteration 155, loss = 0.13099935
Iteration 156, loss = 0.13099052
Iteration 157, loss = 0.13065268
Iteration 158, loss = 0.13069944
Iteration 159, loss = 0.13078671
Iteration 160, loss = 0.13052959
Iteration 161, loss = 0.13046118
Iteration 162, loss = 0.13042185
Iteration 163, loss = 0.13043575
Iteration 164, loss = 0.13035319
Iteration 165, loss = 0.13028903
Iteration 166, loss = 0.13021063
Iteration 167, loss = 0.13015722
Iteration 168, loss = 0.13006517
Iteration 169, loss = 0.13005105
Iteration 170, loss = 0.13005082
Iteration 171, loss = 0.13019381
Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.
Accuracy: 0.9518038962578538
######TRAIN VAL LOSS PLOT DONE######
######CONFUSION MATRIX PLOT DONE######
True Negatives (TN): 214185
False Positives (FP): 10582
False Negatives (FN): 395
True Positives (TP): 2595
Precision: 0.19693405175684905
Recall: 0.8678929765886287
F1-Score: 0.3210243087771386
ROC AUC: 0.910406555830919
######ROC-AUC PLOT DONE######
