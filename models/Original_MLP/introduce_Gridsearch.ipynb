{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Try Grid Search on sklearn\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3b06d5f2ee5b15be"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'kernel': 'linear'}\n",
      "['mean_fit_time', 'mean_score_time', 'mean_test_score', 'param_C', 'param_kernel', 'params', 'rank_test_score', 'split0_test_score', 'split1_test_score', 'split2_test_score', 'split3_test_score', 'split4_test_score', 'std_fit_time', 'std_score_time', 'std_test_score']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Define the parameter grid\n",
    "parameters = {'kernel': ('linear', 'rbf'), 'C': [1, 10]}\n",
    "\n",
    "# Define the model\n",
    "svc = SVC()\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "\n",
    "# Fit the GridSearchCV object to the data\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Output the best parameters\n",
    "print(f\"Best Parameters: {clf.best_params_}\")\n",
    "\n",
    "# Output the sorted cv_results_ keys\n",
    "print(sorted(clf.cv_results_.keys()))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T12:01:34.202323600Z",
     "start_time": "2024-07-24T12:01:34.147325Z"
    }
   },
   "id": "53df0968e63c458a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "torch\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a250d4e99d94b35e"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:07:25.760586600Z",
     "start_time": "2024-07-24T14:07:25.678587400Z"
    }
   },
   "id": "5c2ad6315e5896ae"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_size=4, hidden_size=10, output_size=3, lr=0.01, epochs=10, batch_size=16):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = SimpleNet(input_size, hidden_size, output_size)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:07:25.761586100Z",
     "start_time": "2024-07-24T14:07:25.703586100Z"
    }
   },
   "id": "700bd12cb044e29a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n",
      "[CV] END ...batch_size=16, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=16, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=16, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END ...batch_size=16, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=16, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=16, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=16, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=16, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=10, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=10, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=10, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ...batch_size=32, epochs=20, hidden_size=5, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=5, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=10, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=10, lr=0.001; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END ..batch_size=32, epochs=20, hidden_size=20, lr=0.01; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "[CV] END .batch_size=32, epochs=20, hidden_size=20, lr=0.001; total time=   0.0s\n",
      "Best Parameters: {'batch_size': 32, 'epochs': 20, 'hidden_size': 10, 'lr': 0.01}\n",
      "Best Score: 0.9714285714285714\n",
      "Test Score: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'hidden_size': [5, 10, 20],\n",
    "    'lr': [0.01, 0.001],\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [16, 32]\n",
    "}\n",
    "\n",
    "# Create a PyTorchClassifier instance\n",
    "pytorch_classifier = PyTorchClassifier(input_size=4, output_size=3)\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=pytorch_classifier, param_grid=param_grid, cv=3, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Score:\", test_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T14:07:30.501585200Z",
     "start_time": "2024-07-24T14:07:25.718586700Z"
    }
   },
   "id": "cdb2d4ec2aacb14f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "optim on: \n",
    "\n",
    "lr, epochs, optim, batch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f981603654c8d87"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "data = pd.read_csv(\"data_superstar_v1_0.csv\", delimiter=\",\")\n",
    "data['date'] = pd.to_datetime(data['release_date'])\n",
    "data.sort_values(by=\"date\", inplace=True)\n",
    "\n",
    "\n",
    "# Drop columns not in the list\n",
    "data[\"explicit\"] = data[\"explicit\"].astype(int)\n",
    "\n",
    "\n",
    "def find_min_max(df):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['number'])\n",
    "\n",
    "    # Find max and min values for each numeric column\n",
    "    min_max_values = {}\n",
    "    for col in numeric_cols.columns:\n",
    "        min_value = df[col].min()\n",
    "        max_value = df[col].max()\n",
    "        min_max_values[col] = {'min': min_value, 'max': max_value}\n",
    "\n",
    "    return min_max_values\n",
    "\n",
    "\n",
    "min_max_val = find_min_max(data)\n",
    "\n",
    "y = data[\"hit\"]\n",
    "X = data.drop(columns=[\"hit\", \"song_popularity\", \"date\", \"name_x\", \"name_y\", \"song_id\", \"song_name\", \"artist1_id\", \"artist2_id\", \"song_type\", \"years_on_charts\"])\n",
    "\n",
    "\n",
    "def preprocess(df, min_max_values, exclude_cols=None):\n",
    "    missing_numerical = df.select_dtypes(include=['number']).isnull().sum()\n",
    "    # Fill missing values with mean for each numeric attribute\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    df_filled = df.copy()\n",
    "    for col in missing_numerical.index:\n",
    "        if missing_numerical[col] > 0:\n",
    "            df_filled[col] = imputer.fit_transform(df[[col]])\n",
    "\n",
    "    # Normalize numerical features into [0, 1] range with MinMaxScaler\n",
    "    if exclude_cols:\n",
    "        numerical_cols = df_filled.select_dtypes(include=['number']).columns.difference(exclude_cols)\n",
    "    else:\n",
    "        numerical_cols = df_filled.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # print(\"numerical columns:\", numerical_cols)\n",
    "\n",
    "    for column_name in numerical_cols:\n",
    "        df_filled[column_name] = (df_filled[column_name] - min_max_values[column_name][\"min\"]) / (\n",
    "                min_max_values[column_name][\"max\"] - min_max_values[column_name][\"min\"])\n",
    "\n",
    "    df_normalized = pd.DataFrame(df_filled, columns=numerical_cols)\n",
    "\n",
    "    if exclude_cols:\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns.difference(exclude_cols)\n",
    "    else:\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=False)     #questionable whether true or false\n",
    "\n",
    "    encoded_columns = [col for col in df_encoded.columns if any(orig_col in col for orig_col in categorical_cols)]\n",
    "\n",
    "    #print(categorical_cols)\n",
    "\n",
    "    # Concatenate numerical and encoded categorical features\n",
    "    df_processed = pd.concat([df_normalized, df_encoded[encoded_columns], df[exclude_cols]], axis=1)\n",
    "\n",
    "    return df_processed\n",
    "\n",
    "\n",
    "\n",
    "X_preprocessed = preprocess(X, min_max_val, exclude_cols=[\"release_date\"])\n",
    "X_preprocessed[X_preprocessed.select_dtypes(include=[bool]).columns] = X_preprocessed.select_dtypes(include=[bool]).astype(int)\n",
    "\n",
    "# split_day = X[\"date\"].iloc[-1] - pd.DateOffset(years=1)\n",
    "# X_train = X[(X[\"date\"] < split_day)].copy()\n",
    "# X_test = X[(X[\"date\"] >= split_day)].copy()\n",
    "# \n",
    "# sep_index = X_train.shape[0]\n",
    "# y_train = y.iloc[:sep_index].copy()\n",
    "# y_test = y.iloc[sep_index:].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.25, shuffle=False)\n",
    "\n",
    "feature_names = X_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:05.185851700Z",
     "start_time": "2024-07-24T15:41:54.558248700Z"
    }
   },
   "id": "290622362f4101c2"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_shape[1], 128),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(0.4),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.layers(x)\n",
    "        return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model {path} is loaded from epoch {checkpoint['epoch']} , loss {checkpoint['loss']}\")\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:05.234851900Z",
     "start_time": "2024-07-24T15:42:05.217853500Z"
    }
   },
   "id": "b02f58270292032d"
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "def upsampling(X_train, y_train):\n",
    "    # Convert y_train to a numpy array\n",
    "    # y_train = y_train.to_numpy()\n",
    "    X_train = X_train.to_numpy()\n",
    "\n",
    "    # Count the number of samples in each class\n",
    "    class_counts = np.bincount(y_train.flatten().astype(int))\n",
    "    max_count = class_counts.max()\n",
    "\n",
    "    # Find indices of positive instances\n",
    "    positive_indices = np.where(y_train.flatten() == 1)[0]\n",
    "\n",
    "    # Calculate how many times to duplicate positive samples\n",
    "    difference = max_count - class_counts[1]\n",
    "\n",
    "    # Randomly select indices from positive instances\n",
    "    random_indices = np.random.choice(positive_indices, size=difference, replace=True)\n",
    "\n",
    "    # Get rows corresponding to positive instances\n",
    "    rows_to_duplicate = np.vstack([X_train[idx] for idx in random_indices])\n",
    "\n",
    "    # Stack duplicated rows with the original matrix\n",
    "    X_train_upsampled = np.vstack([X_train, rows_to_duplicate])\n",
    "\n",
    "    # Create an array of shape (x, 1) with all elements as 1\n",
    "    rows_of_ones = np.ones((difference, 1))\n",
    "\n",
    "    # Append rows_of_ones to original_array\n",
    "    y_train_upsampled = np.concatenate((y_train, rows_of_ones), axis=0)\n",
    "\n",
    "    print(\"######UPSAMPLING DONE######\")\n",
    "    return X_train_upsampled, y_train_upsampled"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:05.252851400Z",
     "start_time": "2024-07-24T15:42:05.240849300Z"
    }
   },
   "id": "96501f6b2b37e8e7"
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "#X_train, y_train = upsampling(X_train, y_train.values.reshape(-1, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:05.276850Z",
     "start_time": "2024-07-24T15:42:05.253850800Z"
    }
   },
   "id": "838d618c4d3b69d3"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train, columns=feature_names)\n",
    "y_train_df = pd.DataFrame(y_train, columns=[\"hit\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:14.210664700Z",
     "start_time": "2024-07-24T15:42:05.268852300Z"
    }
   },
   "id": "884bb67a64cc5078"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [],
   "source": [
    "X_train_upsampled_with_y = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "X_train_upsampled_with_y['date'] = pd.to_datetime(X_train_upsampled_with_y['release_date'])\n",
    "X_train_upsampled_with_y.sort_values(by=\"date\", inplace=True)\n",
    "X_train_upsampled_with_y.drop(columns=[\"release_date\", \"date\"], inplace=True)\n",
    "\n",
    "# print(X_train_upsampled_with_y.head())\n",
    "# prepro:\n",
    "y_train_upsampled_ordered = X_train_upsampled_with_y[\"hit\"]\n",
    "X_train_upsampled_ordered = X_train_upsampled_with_y.drop(columns=\"hit\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.324177400Z",
     "start_time": "2024-07-24T15:42:14.208664700Z"
    }
   },
   "id": "b7680b58170470b"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "class PyTorchClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, input_size=4, lr=0.01, epochs=10, batch_size=16):\n",
    "        self.input_size = input_size\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = MLPClassifier(input_size)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        loader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            for batch_X, batch_y in loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = self.model(X_tensor)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.numpy()\n",
    "\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.339174700Z",
     "start_time": "2024-07-24T15:42:17.336176300Z"
    }
   },
   "id": "450c736394944e35"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "X_train_upsampled_ordered = X_train_upsampled_ordered.astype(\"float32\")\n",
    "#X_train_upsampled_ordered = X_train_upsampled_ordered.to_numpy()\n",
    "y_train_upsampled_ordered = y_train_upsampled_ordered.to_frame()\n",
    "y_train_upsampled_ordered = y_train_upsampled_ordered.astype(\"float32\")\n",
    "#y_train_upsampled_ordered = y_train_upsampled_ordered.to_numpy().reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.449176800Z",
     "start_time": "2024-07-24T15:42:17.345178600Z"
    }
   },
   "id": "e2b22b0fc8425085"
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 683270 entries, 0 to 683433\n",
      "Data columns (total 70 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   Cluster_x               683270 non-null  float32\n",
      " 1   Cluster_y               683270 non-null  float32\n",
      " 2   acousticness            683270 non-null  float32\n",
      " 3   artist1_num             683270 non-null  float32\n",
      " 4   artist2_num             683270 non-null  float32\n",
      " 5   betweenesscentrality_x  683270 non-null  float32\n",
      " 6   betweenesscentrality_y  683270 non-null  float32\n",
      " 7   closnesscentrality_x    683270 non-null  float32\n",
      " 8   closnesscentrality_y    683270 non-null  float32\n",
      " 9   clustering_x            683270 non-null  float32\n",
      " 10  clustering_y            683270 non-null  float32\n",
      " 11  danceability            683270 non-null  float32\n",
      " 12  degree_x                683270 non-null  float32\n",
      " 13  degree_y                683270 non-null  float32\n",
      " 14  duration_ms             683270 non-null  float32\n",
      " 15  eccentricity_x          683270 non-null  float32\n",
      " 16  eccentricity_y          683270 non-null  float32\n",
      " 17  eigencentrality_x       683270 non-null  float32\n",
      " 18  eigencentrality_y       683270 non-null  float32\n",
      " 19  energy                  683270 non-null  float32\n",
      " 20  explicit                683270 non-null  float32\n",
      " 21  hits_in_past_x          683270 non-null  float32\n",
      " 22  hits_in_past_y          683270 non-null  float32\n",
      " 23  instrumentalness        683270 non-null  float32\n",
      " 24  key                     683270 non-null  float32\n",
      " 25  liveness                683270 non-null  float32\n",
      " 26  loudness                683270 non-null  float32\n",
      " 27  mode                    683270 non-null  float32\n",
      " 28  num_artists             683270 non-null  float32\n",
      " 29  num_available_markets   683270 non-null  float32\n",
      " 30  pagerank_x              683270 non-null  float32\n",
      " 31  pagerank_y              683270 non-null  float32\n",
      " 32  speechiness             683270 non-null  float32\n",
      " 33  success_rate_x          683270 non-null  float32\n",
      " 34  success_rate_y          683270 non-null  float32\n",
      " 35  superstar_v1_x          683270 non-null  float32\n",
      " 36  superstar_v1_y          683270 non-null  float32\n",
      " 37  superstar_v2_x          683270 non-null  float32\n",
      " 38  superstar_v2_y          683270 non-null  float32\n",
      " 39  superstar_v3_x          683270 non-null  float32\n",
      " 40  superstar_v3_y          683270 non-null  float32\n",
      " 41  superstar_v4_x          683270 non-null  float32\n",
      " 42  superstar_v4_y          683270 non-null  float32\n",
      " 43  superstar_v5_x          683270 non-null  float32\n",
      " 44  superstar_v5_y          683270 non-null  float32\n",
      " 45  superstar_x             683270 non-null  float32\n",
      " 46  superstar_y             683270 non-null  float32\n",
      " 47  tempo                   683270 non-null  float32\n",
      " 48  time_signature          683270 non-null  float32\n",
      " 49  track_number            683270 non-null  float32\n",
      " 50  valence                 683270 non-null  float32\n",
      " 51  weighted degree_x       683270 non-null  float32\n",
      " 52  weighted degree_y       683270 non-null  float32\n",
      " 53  profile_x_1A 2A 3A 4A   683270 non-null  float32\n",
      " 54  profile_x_1A 2A 3A 4B   683270 non-null  float32\n",
      " 55  profile_x_1A 2A 3B 4A   683270 non-null  float32\n",
      " 56  profile_x_1A 2B 3A 4A   683270 non-null  float32\n",
      " 57  profile_x_1A 2B 3A 4B   683270 non-null  float32\n",
      " 58  profile_x_1A 2B 3B 4A   683270 non-null  float32\n",
      " 59  profile_x_1B 2A 3A 4A   683270 non-null  float32\n",
      " 60  profile_x_1B 2B 3A 4A   683270 non-null  float32\n",
      " 61  profile_x_No Match      683270 non-null  float32\n",
      " 62  profile_y_1A 2A 3A 4A   683270 non-null  float32\n",
      " 63  profile_y_1A 2A 3A 4B   683270 non-null  float32\n",
      " 64  profile_y_1A 2A 3B 4A   683270 non-null  float32\n",
      " 65  profile_y_1A 2B 3A 4A   683270 non-null  float32\n",
      " 66  profile_y_1A 2B 3A 4B   683270 non-null  float32\n",
      " 67  profile_y_1A 2B 3B 4A   683270 non-null  float32\n",
      " 68  profile_y_1B 2A 3A 4A   683270 non-null  float32\n",
      " 69  profile_y_No Match      683270 non-null  float32\n",
      "dtypes: float32(70)\n",
      "memory usage: 187.7 MB\n"
     ]
    }
   ],
   "source": [
    "X_train_upsampled_ordered.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.561686Z",
     "start_time": "2024-07-24T15:42:17.453176900Z"
    }
   },
   "id": "5262fe91b5431f48"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "X_train_upsampled_ordered = X_train_upsampled_ordered.to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.626686Z",
     "start_time": "2024-07-24T15:42:17.559689300Z"
    }
   },
   "id": "8ee7b9dc2abd4be4"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "y_train_upsampled_ordered = y_train_upsampled_ordered.to_numpy().reshape(-1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-24T15:42:17.649685Z",
     "start_time": "2024-07-24T15:42:17.623689100Z"
    }
   },
   "id": "c1f109564fe1afc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END .................batch_size=256, epochs=10, lr=0.01; total time= 3.5min\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    #'hidden_size': [5, 10, 20],\n",
    "    'lr': [0.01, 0.001],\n",
    "    'epochs': [10, 20],\n",
    "    'batch_size': [256]\n",
    "}\n",
    "\n",
    "# Create a PyTorchClassifier instance\n",
    "pytorch_classifier = PyTorchClassifier(input_size=list(X_train_upsampled_ordered.shape))\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=pytorch_classifier, param_grid=param_grid, cv=3, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train_upsampled_ordered, y_train_upsampled_ordered)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test data\n",
    "best_model = grid_search.best_estimator_\n",
    "test_score = best_model.score(X_test, y_test)\n",
    "print(\"Test Score:\", test_score)\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-07-24T15:42:17.640689400Z"
    }
   },
   "id": "74b6852b417243e4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "no upsampling because it leads to leakage"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e633044223d819bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nsodkapüskdsä"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c284ee3c4f9b984d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "on ds for real"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4f1e3203739cbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# convert to Pytorch tensor\n",
    "X_train = torch.tensor(X_train.to_numpy(), dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.to_numpy(), dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "print(\"######CONVERSION TO TENSOR######\")\n",
    "\n",
    "# Move the data to the GPU if available\n",
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "198b0e9c63acfc4a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define model\n",
    "print(X_train.size())\n",
    "model = MLPClassifier(X_train.size()).to(device)\n",
    "\n",
    "# Define loss function and optimizer (same as TensorFlow example)\n",
    "loss_fn = nn.BCELoss()  # alternative #BCELoss(weights=weights)#nn.MSELoss()\n",
    "loss_fn_mae = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())#, weight_decay=0.001) # l2 reg\n",
    "#l1_penalty = torch.nn.L1Loss(size_average=False)\n",
    "\n",
    "# Create DataLoader with oversampled data\n",
    "dataset_train = TensorDataset(X_train, y_train)\n",
    "trainloader = DataLoader(dataset_train, batch_size=256, shuffle=True)#, num_workers=2)#, num_workers=2, pin_memory=True) #last two are new look at later\n",
    "dataset_test = TensorDataset(X_test, y_test)\n",
    "val_loader = DataLoader(dataset_test, batch_size=256, shuffle=False)#, num_workers=2)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "val_prec = []\n",
    "epochs = 10\n",
    "best_val_loss = 1e8\n",
    "best_val_acc = 0\n",
    "best_precision = 0\n",
    "best_prec_acc = 0.0\n",
    "version = \"v3_try_reg\"\n",
    "\n",
    "lambda1 = 0.001  # L1 penalty strength\n",
    "nweights = 0\n",
    "for name,weights in model.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        nweights = nweights + weights.numel()\n",
    "print(f'Total number of weights in the model = {nweights}')\n",
    "\n",
    "for epoch in range(epochs):  # Adjust epochs as needed\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_val_loss = 0.0\n",
    "\n",
    "    # Initialize counts for each class\n",
    "\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    for X_batch, y_batch in trainloader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        # print(\"y_batch: \", y_batch)\n",
    "        # print(\"y_pred: \", y_pred)\n",
    "        #print(y_batch.shape)\n",
    "        y_batch = y_batch.reshape(-1, 1)\n",
    "        #print(y_batch.shape)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        #l1_norm = 0\n",
    "        #for param in model.parameters():\n",
    "        #    l1_norm += l1_penalty(param)\n",
    "        #l2_norm = sum(torch.square(param) for param in model.parameters())\n",
    "        #l1_norm = sum(param.abs().sum() for param in model.parameters())\n",
    "        #l2_norm = sum(param.pow(2).sum() for param in model.parameters())        \n",
    "        #l1_penalty_var = lambda1 * l1_norm #+ lambda2 * l2_norm\n",
    "        #total_loss = loss + l1_penalty_var\n",
    "        L1_term = torch.tensor(0., requires_grad=True)\n",
    "        for name, weights in model.named_parameters():\n",
    "            if 'bias' not in name:\n",
    "                weights_sum = torch.sum(torch.abs(weights))\n",
    "                L1_term = L1_term + weights_sum\n",
    "        L1_term = L1_term / nweights\n",
    "\n",
    "        # l2_weight = 0.001\n",
    "        # l2_parameters = []\n",
    "        # for parameter in model.parameters():\n",
    "        #     l2_parameters.append(parameter.view(-1))\n",
    "        # l2 = l2_weight * torch.square(torch.cat(l2_parameters)).sum()\n",
    "\n",
    "        # Regularize loss using L1 regularization\n",
    "        total_loss = loss #+ L1_term * lambda1 #+ l2 # -  ?\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_train_loss += total_loss.item()\n",
    "    # Calculate average epoch training loss\n",
    "    avg_epoch_train_loss = epoch_train_loss / len(trainloader)\n",
    "    train_losses.append(avg_epoch_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0.0, 0.0\n",
    "        tp, fp =  0.0, 0.0\n",
    "        loss_step = []\n",
    "        for data in val_loader:\n",
    "            inp_data, labels = data\n",
    "            inp_data = inp_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inp_data)\n",
    "            labels = labels.reshape(-1, 1)\n",
    "            val_loss = loss_fn(outputs, labels)\n",
    "            predicted = outputs.round()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum()\n",
    "            loss_step.append(val_loss.item())\n",
    "            #print(labels.shape)\n",
    "            #print(predicted.shape)\n",
    "            tp  += ((labels == 1) & (1 == predicted)).sum().item()\n",
    "            fp += ((labels == 0) & (1 == predicted)).sum().item()\n",
    "        # dont forget to take the means here\n",
    "        epoch_val_acc = (correct / total).cpu().numpy()\n",
    "        epoch_val_loss = torch.tensor(loss_step).mean().numpy()\n",
    "        #print('TP:', tp, 'FP:', fp)\n",
    "        epoch_val_prec = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "            }, f'best_torch_{version}_model_min_val_loss.pth')\n",
    "        if epoch_val_acc > best_val_acc:\n",
    "            best_val_acc = epoch_val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "            }, f'best_torch_{version}_model_max_val_acc.pth')\n",
    "        if epoch_val_prec > best_precision or (epoch_val_acc > best_prec_acc and best_precision == epoch_val_prec):\n",
    "            best_precision = epoch_val_prec\n",
    "            best_prec_acc = epoch_val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': epoch_val_loss,\n",
    "                'precision': best_precision,\n",
    "            }, f\"best_torch_{version}_model_max_val_prec.pth\")\n",
    "\n",
    "        val_prec.append(epoch_val_prec)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        val_accs.append(epoch_val_acc)\n",
    "        print(\n",
    "            f\"Epoch [{epoch + 1}/{epochs}], Training Loss: {avg_epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation Accuracy: {epoch_val_acc:.4f}, Validation Precision: {epoch_val_prec:.4f}\")\n",
    "\n",
    "print(\"######TRAINING DONE######\")\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    checkpoint = torch.load(path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print(f\"Model {path} is loaded from epoch {checkpoint['epoch']} , loss {checkpoint['loss']}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"######LOAD MODEL######\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MLPClassifier(X_train.size())\n",
    "model = load_model(model, f\"best_torch_{version}_model_max_val_prec.pth\")\n",
    "model = model.to(device)\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "e73a81eea855ee40"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training loss\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss vs. Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(f\"losses_pytorch_{version}.png\")\n",
    "print(\"######LOSS PLOT DONE######\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "output = model(X_test)\n",
    "# print(\"output\", output)\n",
    "\n",
    "opt_thres = -1\n",
    "opt_prec = 0\n",
    "liste_thresh = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "true_labels = y_test.int().tolist()\n",
    "# print(output.tolist())\n",
    "for i in liste_thresh:\n",
    "    flattened_list = [item for sublist in output.tolist() for item in sublist]\n",
    "    predictions = list(map(lambda x: int(x >= i), flattened_list))\n",
    "\n",
    "    precision = metrics.precision_score(true_labels, predictions)\n",
    "\n",
    "    # Recall\n",
    "    recall = metrics.recall_score(true_labels, predictions)\n",
    "    # F1-Score\n",
    "    f1 = metrics.f1_score(true_labels, predictions)\n",
    "    # ROC Curve and AUC\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    # print(\"Precision:\", precision)\n",
    "    # print(\"Recall:\", recall)\n",
    "    # print(\"F1-Score:\", f1)\n",
    "    # print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "    if precision > opt_prec:\n",
    "        opt_thres = i\n",
    "        opt_prec = precision\n",
    "print(f\"optimal threshold {opt_thres}, with precision {opt_prec}\")\n",
    "\n",
    "predictions = output.round().int().tolist()  # Converting tensor to list of integers\n",
    "true_labels = y_test.int().tolist()  # Converting tensor to list of integers\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(true_labels, predictions)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix=confusion_matrix, display_labels=[False, True])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.savefig(f\"Confusion_Matrix_pytorch_{version}.png\")\n",
    "print(\"######CONFUSION MATRIX PLOT DONE######\")\n",
    "\n",
    "# Extract TN, FP, TP values\n",
    "TN = confusion_matrix[0, 0]  # True Negatives\n",
    "FP = confusion_matrix[0, 1]  # False Positives\n",
    "FN = confusion_matrix[1, 0]  # False Negatives\n",
    "TP = confusion_matrix[1, 1]  # True Positives\n",
    "\n",
    "# Print the results\n",
    "print(\"True Negatives (TN):\", TN)\n",
    "print(\"False Positives (FP):\", FP)\n",
    "print(\"False Negatives (FN):\", FN)\n",
    "print(\"True Positives (TP):\", TP)\n",
    "\n",
    "# Precision \n",
    "precision = metrics.precision_score(true_labels, predictions)\n",
    "# Recall \n",
    "recall = metrics.recall_score(true_labels, predictions)\n",
    "# F1-Score \n",
    "f1 = metrics.f1_score(true_labels, predictions)\n",
    "# ROC Curve and AUC \n",
    "fpr, tpr, thresholds = metrics.roc_curve(true_labels, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-Score:\", f1)\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "\n",
    "# print(output.device)\n",
    "output_cpu = output.cpu().detach().numpy()\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test.tolist(), output_cpu.tolist())\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(f\"ROC_AUC_pytorch_{version}.png\")\n",
    "print(\"######ROC-AUC PLOT DONE######\")\n",
    "\n",
    "# Generate a classification report\n",
    "class_report = classification_report(y_test.tolist(), predictions)\n",
    "print(\"Classification Report:\\n\", class_report)\n",
    "\n",
    "y_test_cpu = y_test.cpu()\n",
    "y_test_list = y_test_cpu.tolist()\n",
    "\n",
    "# Convert predictions to list\n",
    "predictions_list = list(np.hstack(predictions))\n",
    "\n",
    "y_test_series = pd.Series(list(np.hstack(y_test_list)))\n",
    "count_occ = y_test_series.value_counts(normalize=True)\n",
    "\n",
    "# Calculate the weighted accuracy\n",
    "weighted_acc = (np.sum((y_test_series == 1) == predictions_list) * count_occ[0] + np.sum(\n",
    "    (y_test_series == 0) == predictions_list) * count_occ[1]) / len(y_test_list)\n",
    "\n",
    "print(\"Weighted Accuracy:\", weighted_acc)\n",
    "\n",
    "macro_f1 = metrics.f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5cdbff63ced1cb1d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
