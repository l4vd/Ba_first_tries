{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_hits = pd.read_csv(\"additional/hits_dataset.csv\", delimiter=\"\\t\")\n",
    "df_hits[\"hit\"] = 1\n",
    "df_hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nonhits = pd.read_csv(\"additional/nonhits_dataset.csv\", delimiter=\"\\t\")\n",
    "df_nonhits[\"hit\"] = 0\n",
    "df_nonhits"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "26d5e57f02f1e85d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df_hits, df_nonhits], axis=0)\n",
    "combined_df.replace(\"-\", float(\"nan\"), inplace=True)\n",
    "# Drop all rows with NaN values\n",
    "combined_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6f9b2d1a6293f615"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for NaN values in all columns\n",
    "nan_columns = combined_df.columns[combined_df.isna().any()].tolist()\n",
    "\n",
    "# Print columns with NaN values\n",
    "print(\"Columns with NaN values:\")\n",
    "print(nan_columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8756697fa413e9af"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "combined_df.replace(\"-\", float(\"nan\"), inplace=True)   ## evtl hier schon mean replacement???\n",
    "# combined_df.dropna(inplace=True)   # drop nan value rows\n",
    "#Fill missing values with mean for each numeric attribute\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "for col in nan_columns:\n",
    "    combined_df[col] = imputer.fit_transform(combined_df[[col]])\n",
    "\n",
    "combined_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "723e4477181f9728"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check for NaN values in all columns\n",
    "nan_columns = combined_df.columns[combined_df.isna().any()].tolist()\n",
    "\n",
    "# Print columns with NaN values\n",
    "print(\"Columns with NaN values:\")\n",
    "print(nan_columns)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ad978e73fb721d3e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "combined_df['date'] = pd.to_datetime(combined_df['release_date'])\n",
    "combined_df.sort_values(by=\"date\", inplace=True)\n",
    "border_day = combined_df[\"date\"].iloc[-1]  - pd.DateOffset(years=6)\n",
    "combined_df = combined_df[(combined_df[\"date\"] >= border_day)]\n",
    "\n",
    "split_day = combined_df[\"date\"].iloc[-1]  - pd.DateOffset(years=1)\n",
    "train_df = combined_df[(combined_df[\"date\"]  < split_day)]\n",
    "test_df = combined_df[(combined_df[\"date\"]  >= split_day)]\n",
    "#train_df, test_df = train_test_split(combined_df, test_size=1/6, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "f55fc8cca1de5d29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "combined_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e4154b39a1657b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the columns\n",
    "columns = ['artist_id', 'artist_name']\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df_artists_train = pd.DataFrame(columns={col: [] for col in columns})\n",
    "\n",
    "count = 0\n",
    "artist_set = set()\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    current_artist_id_list = eval(row[\"id_artists\"])\n",
    "    current_artist_name_list = eval(row[\"name_artists\"])\n",
    "    for cur_art_id, cur_art_name in zip(current_artist_id_list, current_artist_name_list):\n",
    "        if cur_art_id not in artist_set:\n",
    "            artist_set.add(cur_art_id)\n",
    "            df_artists_train.loc[len(df_artists_train)] = [cur_art_id, cur_art_name]\n",
    "            count += 1\n",
    "\n",
    "print(\"amount of artists:\", count)\n",
    "df_artists_train"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4e89192b5a6cf7c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "all_artist_ids = artist_set.copy()\n",
    "\n",
    "G = nx.Graph()\n",
    "\n",
    "for id_art_cur in all_artist_ids:\n",
    "    artist_name = df_artists_train[(df_artists_train[\"artist_id\"] == id_art_cur)][\"artist_name\"]\n",
    "    artist_info = df_artists_train[df_artists_train[\"artist_id\"] == id_art_cur]\n",
    "    if not artist_info.empty:  # Check if artist info exists\n",
    "        # Filter out artists who are not successful\n",
    "        node_attrs = {\"name\": artist_name.iloc[0]}\n",
    "        G.add_node(id_art_cur, **node_attrs)\n",
    "    else:\n",
    "        print(\"Artist info not found for ID:\", id_art_cur)\n",
    "\n",
    "for _, row in train_df.iterrows():\n",
    "    artist_ids = list(eval(row[\"id_artists\"]))\n",
    "    artist_names = list(eval(row[\"name_artists\"]))\n",
    "    # Filter out songs that do not have more than one artist in their execution\n",
    "    if len(artist_ids) > 1:\n",
    "        for i in range(len(artist_ids)):\n",
    "            for j in range(i+1, len(artist_ids)):\n",
    "                artist_id_1 = str(artist_ids[i])\n",
    "                lable1 = str(artist_names[i])\n",
    "                artist_id_2 = str(artist_ids[j])\n",
    "                lable2 = str(artist_names[j])\n",
    "                # Check if the edge already exists\n",
    "                if G.has_edge(artist_id_1, artist_id_2):\n",
    "                    # If the edge already exists, increment the weight by 1\n",
    "                    G[artist_id_1][artist_id_2]['weight'] += 2\n",
    "                else:\n",
    "                    # Add a new edge with weight 1\n",
    "                    G.add_edge(artist_id_1, artist_id_2, lable1=lable1, lable2=lable2, weight=2, song_id=str(row[\"song_id\"]), song_name=str(row[\"song_name\"]), explicit=bool(row[\"explicit\"]), song_type=str(row[\"song_type\"]), track_number=int(row[\"track_number\"]), num_artists=int(row[\"num_artists\"]), num_available_markets=int(row[\"num_available_markets\"]), duration_ms=int(row[\"duration_ms\"]), popularity=int(row[\"popularity\"]), release_date=str(row[\"release_date\"]), key = int(row[\"key\"]), mode = int(row[\"mode\"]), time_signature = int(row[\"time_signature\"]), acousticness = float(row[\"acousticness\"]), danceability = float(row[\"danceability\"]), energy = float(row[\"energy\"]), instrumentalness = float(row[\"instrumentalness\"]), liveness = float(row[\"liveness\"]),loudness = float(row[\"loudness\"]), speechiness = float(row[\"speechiness\"]), valence = float(row[\"valence\"]), tempo = float(row[\"tempo\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "13bb7f1a4a4876a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming G is your NetworkX graph\n",
    "\n",
    "# Extract nodes and their attributes from the graph\n",
    "nodes_with_attributes = [(G.nodes[node], node) for node in G.nodes()]\n",
    "\n",
    "# Create a DataFrame from the nodes and their attributes\n",
    "df_nodes_train = pd.DataFrame(nodes_with_attributes, columns=['Attributes', \"Spotify ID\"])\n",
    "\n",
    "# Normalize the 'Attributes' column to expand it into separate columns\n",
    "df_attributes = pd.json_normalize(df_nodes_train['Attributes'])\n",
    "\n",
    "# Combine the original 'Node' column with the expanded attributes\n",
    "df_nodes_train = pd.concat([df_attributes, df_nodes_train['Spotify ID']], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_nodes_train"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a02e5a328f587e87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nodes_train.to_csv(\"network_created/nodes_real_train.csv\", sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "affbd668a36e7e8a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract edges and their attributes from the graph\n",
    "edges_with_attributes = [(u, v, G.edges[u, v]) for u, v in G.edges()]\n",
    "\n",
    "# Create a DataFrame from the edges and their attributes\n",
    "df_edges_train = pd.DataFrame(edges_with_attributes, columns=['Source', 'Target', 'Attributes'])\n",
    "\n",
    "# Normalize the 'Attributes' column to expand it into separate columns\n",
    "df_edge_attributes = pd.json_normalize(df_edges_train['Attributes'])\n",
    "\n",
    "# Combine the original 'Attributes' column with the expanded attributes\n",
    "df_edges_train = pd.concat([df_edges_train[['Source', 'Target']], df_edge_attributes], axis=1)\n",
    "df_edges_train"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b94ca6391b6c88f8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save edge DataFrame to CSV\n",
    "df_edges_train.to_csv('network_created/edges_real_train.csv', sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "8cf334b81ab892a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ed2d73175e8f4ff7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Specify the columns\n",
    "columns = ['artist_id', 'artist_name']\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "df_artists_test = pd.DataFrame(columns={col: [] for col in columns})\n",
    "\n",
    "count = 0\n",
    "artist_set = set()\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    current_artist_id_list = eval(row[\"id_artists\"])\n",
    "    current_artist_name_list = eval(row[\"name_artists\"])\n",
    "    for cur_art_id, cur_art_name in zip(current_artist_id_list, current_artist_name_list):\n",
    "        if cur_art_id not in artist_set:\n",
    "            artist_set.add(cur_art_id)\n",
    "            df_artists_test.loc[len(df_artists_test)] = [cur_art_id, cur_art_name]\n",
    "            count += 1\n",
    "\n",
    "print(\"amount of artists:\", count)\n",
    "df_artists_test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "debfdb478f00e0a8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "all_artist_ids = artist_set.copy()\n",
    "\n",
    "G2 = nx.Graph()\n",
    "\n",
    "for id_art_cur in all_artist_ids:\n",
    "    artist_name = df_artists_test[(df_artists_test[\"artist_id\"] == id_art_cur)][\"artist_name\"]\n",
    "    artist_info = df_artists_test[df_artists_test[\"artist_id\"] == id_art_cur]\n",
    "    if not artist_info.empty:  # Check if artist info exists\n",
    "        # Filter out artists who are not successful\n",
    "        node_attrs = {\"name\": artist_name.iloc[0]}\n",
    "        G2.add_node(id_art_cur, **node_attrs)\n",
    "    else:\n",
    "        print(\"Artist info not found for ID:\", id_art_cur)\n",
    "\n",
    "for _, row in test_df.iterrows():\n",
    "    artist_ids = list(eval(row[\"id_artists\"]))\n",
    "    artist_names = list(eval(row[\"name_artists\"]))\n",
    "    # Filter out songs that do not have more than one artist in their execution\n",
    "    if len(artist_ids) > 1:\n",
    "        for i in range(len(artist_ids)):\n",
    "            for j in range(i+1, len(artist_ids)):\n",
    "                artist_id_1 = str(artist_ids[i])\n",
    "                lable1 = str(artist_names[i])\n",
    "                artist_id_2 = str(artist_ids[j])\n",
    "                lable2 = str(artist_names[j])\n",
    "                # Check if the edge already exists\n",
    "                if G2.has_edge(artist_id_1, artist_id_2):\n",
    "                    # If the edge already exists, increment the weight by 1\n",
    "                    G2[artist_id_1][artist_id_2]['weight'] += 2\n",
    "                else:\n",
    "                    # Add a new edge with weight 1\n",
    "                    G2.add_edge(artist_id_1, artist_id_2, lable1=lable1, lable2=lable2, weight=2, song_id=str(row[\"song_id\"]), song_name=str(row[\"song_name\"]), explicit=bool(row[\"explicit\"]), song_type=str(row[\"song_type\"]), track_number=int(row[\"track_number\"]), num_artists=int(row[\"num_artists\"]), num_available_markets=int(row[\"num_available_markets\"]), duration_ms=int(row[\"duration_ms\"]), popularity=int(row[\"popularity\"]), release_date=str(row[\"release_date\"]), key = int(row[\"key\"]), mode = int(row[\"mode\"]), time_signature = int(row[\"time_signature\"]), acousticness = float(row[\"acousticness\"]), danceability = float(row[\"danceability\"]), energy = float(row[\"energy\"]), instrumentalness = float(row[\"instrumentalness\"]), liveness = float(row[\"liveness\"]),loudness = float(row[\"loudness\"]), speechiness = float(row[\"speechiness\"]), valence = float(row[\"valence\"]), tempo = float(row[\"tempo\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "fe0fb448adf18ca8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assuming G is your NetworkX graph\n",
    "\n",
    "# Extract nodes and their attributes from the graph\n",
    "nodes_with_attributes = [(G2.nodes[node], node) for node in G2.nodes()]\n",
    "\n",
    "# Create a DataFrame from the nodes and their attributes\n",
    "df_nodes_test = pd.DataFrame(nodes_with_attributes, columns=['Attributes', \"Spotify ID\"])\n",
    "\n",
    "# Normalize the 'Attributes' column to expand it into separate columns\n",
    "df_attributes = pd.json_normalize(df_nodes_test['Attributes'])\n",
    "\n",
    "# Combine the original 'Node' column with the expanded attributes\n",
    "df_nodes_test = pd.concat([df_attributes, df_nodes_test['Spotify ID']], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_nodes_test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4b5d80bd0f928513"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nodes_test.to_csv(\"network_created/nodes_real_test.csv\", sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eeab99250e649d7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract edges and their attributes from the graph\n",
    "edges_with_attributes = [(u, v, G2.edges[u, v]) for u, v in G2.edges()]\n",
    "\n",
    "# Create a DataFrame from the edges and their attributes\n",
    "df_edges_test = pd.DataFrame(edges_with_attributes, columns=['Source', 'Target', 'Attributes'])\n",
    "\n",
    "# Normalize the 'Attributes' column to expand it into separate columns\n",
    "df_edge_attributes = pd.json_normalize(df_edges_test['Attributes'])\n",
    "\n",
    "# Combine the original 'Attributes' column with the expanded attributes\n",
    "df_edges_test = pd.concat([df_edges_test[['Source', 'Target']], df_edge_attributes], axis=1)\n",
    "df_edges_test"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2e6324973210ec7e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save edge DataFrame to CSV\n",
    "df_edges_test.to_csv('network_created/edges_real_test.csv', sep='\\t', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b2a4471b27c98a30"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_nodes = G2.number_of_nodes()\n",
    "num_edges = G2.number_of_edges()\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b6287a821fd97ef0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b1de9789d17182a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "4aa2a74a75195a81"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
