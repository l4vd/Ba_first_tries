{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:06.675472400Z",
     "start_time": "2024-02-21T16:27:05.847469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      song_id                                      song_name  \\\n0      3e9HZxeyfWwjeyPAMmWSSQ                                  thank u, next   \n1      5p7ujcrUXASCNwRaWNHR1C                                     Without Me   \n2      2xLMifQCjDGFmkHkpNLD9h                                     SICKO MODE   \n3      3KkXRkHbMCARz0aVfEt68P  Sunflower - Spider-Man: Into the Spider-Verse   \n4      1rqqCSm0Qe4I9rUvWncaom                                     High Hopes   \n...                       ...                                            ...   \n20400  4NnhLA66RRLXxKbiiscU9R             How Can I Be Sure - Single Version   \n20401  2jHfXdCLibrI1J56LnUAZv                      To Show I Love You - Mono   \n20402  6zqsyB7uIvWrL1iCJzpNrs                You Better Run - Single Version   \n20403  5mz9pQZZXNpAw9CdQ7Bk8q                           Don't Pity Me - Mono   \n20404  2H9CKpZiLDF223BbwehpDF                                      Hush Hush   \n\n                                               billboard  \\\n0                     ('Thank U, Next', 'Ariana Grande')   \n1                               ('Without Me', 'Halsey')   \n2                         ('Sicko Mode', 'Travis Scott')   \n3      ('Sunflower (Spider-Man: Into The Spider-Verse...   \n4                  ('High Hopes', 'Panic! At The Disco')   \n...                                                  ...   \n20400             ('How Can I Be Sure', 'David Cassidy')   \n20401         ('To Show I Love You', 'Peter And Gordon')   \n20402                  ('You Better Run', 'Pat Benatar')   \n20403              ('Don't Pity Me', 'Peter And Gordon')   \n20404  ('Hush Hush', 'The Pussycat Dolls Featuring Ni...   \n\n                                                 artists  popularity  \\\n0            {'66CXWjxzNUsdJxJ2JdwvnR': 'Ariana Grande'}          86   \n1                   {'26VFTg2z8YR0cCuwLzESi2': 'Halsey'}          87   \n2             {'0Y5tJX1MQlPlqiwlOH1tJY': 'Travis Scott'}          85   \n3      {'246dkjvS1zLTtiykXe5h60': 'Post Malone', '1zN...          92   \n4      {'20JZFwl6HVl6yg8a4H3ZqK': 'Panic! At The Disco'}          86   \n...                                                  ...         ...   \n20400    {'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}          36   \n20401     {'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}           1   \n20402    {'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}          20   \n20403     {'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}           7   \n20404   {'6wPhSqRtPu1UhRCDX5yaDJ': 'The Pussycat Dolls'}          57   \n\n       explicit      song_type  \n0          True           Solo  \n1          True           Solo  \n2          True           Solo  \n3         False  Collaboration  \n4         False           Solo  \n...         ...            ...  \n20400     False           Solo  \n20401     False           Solo  \n20402     False           Solo  \n20403     False           Solo  \n20404     False           Solo  \n\n[20405 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>song_id</th>\n      <th>song_name</th>\n      <th>billboard</th>\n      <th>artists</th>\n      <th>popularity</th>\n      <th>explicit</th>\n      <th>song_type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3e9HZxeyfWwjeyPAMmWSSQ</td>\n      <td>thank u, next</td>\n      <td>('Thank U, Next', 'Ariana Grande')</td>\n      <td>{'66CXWjxzNUsdJxJ2JdwvnR': 'Ariana Grande'}</td>\n      <td>86</td>\n      <td>True</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n      <td>Without Me</td>\n      <td>('Without Me', 'Halsey')</td>\n      <td>{'26VFTg2z8YR0cCuwLzESi2': 'Halsey'}</td>\n      <td>87</td>\n      <td>True</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n      <td>SICKO MODE</td>\n      <td>('Sicko Mode', 'Travis Scott')</td>\n      <td>{'0Y5tJX1MQlPlqiwlOH1tJY': 'Travis Scott'}</td>\n      <td>85</td>\n      <td>True</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3KkXRkHbMCARz0aVfEt68P</td>\n      <td>Sunflower - Spider-Man: Into the Spider-Verse</td>\n      <td>('Sunflower (Spider-Man: Into The Spider-Verse...</td>\n      <td>{'246dkjvS1zLTtiykXe5h60': 'Post Malone', '1zN...</td>\n      <td>92</td>\n      <td>False</td>\n      <td>Collaboration</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n      <td>High Hopes</td>\n      <td>('High Hopes', 'Panic! At The Disco')</td>\n      <td>{'20JZFwl6HVl6yg8a4H3ZqK': 'Panic! At The Disco'}</td>\n      <td>86</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20400</th>\n      <td>4NnhLA66RRLXxKbiiscU9R</td>\n      <td>How Can I Be Sure - Single Version</td>\n      <td>('How Can I Be Sure', 'David Cassidy')</td>\n      <td>{'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}</td>\n      <td>36</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>20401</th>\n      <td>2jHfXdCLibrI1J56LnUAZv</td>\n      <td>To Show I Love You - Mono</td>\n      <td>('To Show I Love You', 'Peter And Gordon')</td>\n      <td>{'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}</td>\n      <td>1</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>20402</th>\n      <td>6zqsyB7uIvWrL1iCJzpNrs</td>\n      <td>You Better Run - Single Version</td>\n      <td>('You Better Run', 'Pat Benatar')</td>\n      <td>{'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}</td>\n      <td>20</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>20403</th>\n      <td>5mz9pQZZXNpAw9CdQ7Bk8q</td>\n      <td>Don't Pity Me - Mono</td>\n      <td>('Don't Pity Me', 'Peter And Gordon')</td>\n      <td>{'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}</td>\n      <td>7</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n    <tr>\n      <th>20404</th>\n      <td>2H9CKpZiLDF223BbwehpDF</td>\n      <td>Hush Hush</td>\n      <td>('Hush Hush', 'The Pussycat Dolls Featuring Ni...</td>\n      <td>{'6wPhSqRtPu1UhRCDX5yaDJ': 'The Pussycat Dolls'}</td>\n      <td>57</td>\n      <td>False</td>\n      <td>Solo</td>\n    </tr>\n  </tbody>\n</table>\n<p>20405 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_songs = pd.read_csv(\"musicoset_metadata/songs.csv\", delimiter=\"\\t\")\n",
    "df_songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab563085b08ceb04",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:06.792772200Z",
     "start_time": "2024-02-21T16:27:06.665473500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      song_id                album_id  track_number  \\\n0      3e9HZxeyfWwjeyPAMmWSSQ  2fYhqwDWXjbpjaIJPEfKFw            11   \n1      5p7ujcrUXASCNwRaWNHR1C  0zzrCTzvL4ZmR42xF46Afm             1   \n2      2xLMifQCjDGFmkHkpNLD9h  41GuZcammIkupMPKH2OJ6I             3   \n3      3KkXRkHbMCARz0aVfEt68P  35s58BRTGAEWztPo9WqCIs             2   \n4      1rqqCSm0Qe4I9rUvWncaom  6ApYSpXF8GxZAgBTHDzYge             4   \n...                       ...                     ...           ...   \n20400  0yZsBLZVU2HTMlMqYvWevJ  5UoKanu6wcBQhwfPSDHYVw             4   \n20401  30342v7I9sUoNC0Djnu1mW  1bZHbHtUvjGqUOKNla4lo0            25   \n20402  5rkgbGIOTBGlKTAawWb06X  3GmCXW10kLxmZrEY0JpRlw             9   \n20403  2MVPsa4S4r7Ii5lMh85E03  0UiVuqXa69zXqql0GoXH2h            22   \n20404  7wGbzk3aeGSpnsK2AQ7vzA  3pBArpt3QcnvVj58hl6Ghe             7   \n\n      release_date release_date_precision  \n0       2019-02-08                    day  \n1       2018-10-04                    day  \n2       2018-08-03                    day  \n3       2018-12-14                    day  \n4       2018-06-22                    day  \n...            ...                    ...  \n20400   2011-04-01                    day  \n20401   2000-01-01                    day  \n20402   1988-03-07                    day  \n20403   2013-01-31                    day  \n20404   2012-05-31                    day  \n\n[20405 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>song_id</th>\n      <th>album_id</th>\n      <th>track_number</th>\n      <th>release_date</th>\n      <th>release_date_precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3e9HZxeyfWwjeyPAMmWSSQ</td>\n      <td>2fYhqwDWXjbpjaIJPEfKFw</td>\n      <td>11</td>\n      <td>2019-02-08</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n      <td>0zzrCTzvL4ZmR42xF46Afm</td>\n      <td>1</td>\n      <td>2018-10-04</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n      <td>41GuZcammIkupMPKH2OJ6I</td>\n      <td>3</td>\n      <td>2018-08-03</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3KkXRkHbMCARz0aVfEt68P</td>\n      <td>35s58BRTGAEWztPo9WqCIs</td>\n      <td>2</td>\n      <td>2018-12-14</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n      <td>6ApYSpXF8GxZAgBTHDzYge</td>\n      <td>4</td>\n      <td>2018-06-22</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20400</th>\n      <td>0yZsBLZVU2HTMlMqYvWevJ</td>\n      <td>5UoKanu6wcBQhwfPSDHYVw</td>\n      <td>4</td>\n      <td>2011-04-01</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20401</th>\n      <td>30342v7I9sUoNC0Djnu1mW</td>\n      <td>1bZHbHtUvjGqUOKNla4lo0</td>\n      <td>25</td>\n      <td>2000-01-01</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20402</th>\n      <td>5rkgbGIOTBGlKTAawWb06X</td>\n      <td>3GmCXW10kLxmZrEY0JpRlw</td>\n      <td>9</td>\n      <td>1988-03-07</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20403</th>\n      <td>2MVPsa4S4r7Ii5lMh85E03</td>\n      <td>0UiVuqXa69zXqql0GoXH2h</td>\n      <td>22</td>\n      <td>2013-01-31</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20404</th>\n      <td>7wGbzk3aeGSpnsK2AQ7vzA</td>\n      <td>3pBArpt3QcnvVj58hl6Ghe</td>\n      <td>7</td>\n      <td>2012-05-31</td>\n      <td>day</td>\n    </tr>\n  </tbody>\n</table>\n<p>20405 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tracks = pd.read_csv(\"musicoset_metadata/tracks.csv\", delimiter=\"\\t\")\n",
    "df_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba207b02c2408a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:07.024975Z",
     "start_time": "2024-02-21T16:27:06.722470800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "16\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "3\n",
      "3\n",
      "9\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": "277"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df = pd.merge(df_songs, df_tracks, on='song_id', how='inner')\n",
    "list_release_artists = []\n",
    "amount_multiple = 0\n",
    "for row in merged_df[\"artists\"]:\n",
    "    #list_release_artists.append(eval(row))\n",
    "    if len(eval(row)) > 2:\n",
    "        amount_multiple += 1\n",
    "        print(len(eval(row)))\n",
    "        \n",
    "amount_multiple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df660324e5c52466",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:07.293075200Z",
     "start_time": "2024-02-21T16:27:07.004502700Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df[\"artists\"] = merged_df[\"artists\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ecfa5c2749b557",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:07.345075400Z",
     "start_time": "2024-02-21T16:27:07.302075300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                      song_id                                      song_name  \\\n0      3e9HZxeyfWwjeyPAMmWSSQ                                  thank u, next   \n1      5p7ujcrUXASCNwRaWNHR1C                                     Without Me   \n2      2xLMifQCjDGFmkHkpNLD9h                                     SICKO MODE   \n3      3KkXRkHbMCARz0aVfEt68P  Sunflower - Spider-Man: Into the Spider-Verse   \n4      1rqqCSm0Qe4I9rUvWncaom                                     High Hopes   \n...                       ...                                            ...   \n20400  4NnhLA66RRLXxKbiiscU9R             How Can I Be Sure - Single Version   \n20401  2jHfXdCLibrI1J56LnUAZv                      To Show I Love You - Mono   \n20402  6zqsyB7uIvWrL1iCJzpNrs                You Better Run - Single Version   \n20403  5mz9pQZZXNpAw9CdQ7Bk8q                           Don't Pity Me - Mono   \n20404  2H9CKpZiLDF223BbwehpDF                                      Hush Hush   \n\n                                                 artists  popularity  \\\n0            {'66CXWjxzNUsdJxJ2JdwvnR': 'Ariana Grande'}          86   \n1                   {'26VFTg2z8YR0cCuwLzESi2': 'Halsey'}          87   \n2             {'0Y5tJX1MQlPlqiwlOH1tJY': 'Travis Scott'}          85   \n3      {'246dkjvS1zLTtiykXe5h60': 'Post Malone', '1zN...          92   \n4      {'20JZFwl6HVl6yg8a4H3ZqK': 'Panic! At The Disco'}          86   \n...                                                  ...         ...   \n20400    {'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}          36   \n20401     {'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}           1   \n20402    {'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}          20   \n20403     {'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}           7   \n20404   {'6wPhSqRtPu1UhRCDX5yaDJ': 'The Pussycat Dolls'}          57   \n\n       explicit release_date release_date_precision  \n0          True   2019-02-08                    day  \n1          True   2018-10-04                    day  \n2          True   2018-08-03                    day  \n3         False   2018-12-14                    day  \n4         False   2018-06-22                    day  \n...         ...          ...                    ...  \n20400     False         1967                   year  \n20401     False   2009-11-27                    day  \n20402     False         1967                   year  \n20403     False   2009-11-27                    day  \n20404     False   2009-01-01                    day  \n\n[20405 rows x 7 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>song_id</th>\n      <th>song_name</th>\n      <th>artists</th>\n      <th>popularity</th>\n      <th>explicit</th>\n      <th>release_date</th>\n      <th>release_date_precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3e9HZxeyfWwjeyPAMmWSSQ</td>\n      <td>thank u, next</td>\n      <td>{'66CXWjxzNUsdJxJ2JdwvnR': 'Ariana Grande'}</td>\n      <td>86</td>\n      <td>True</td>\n      <td>2019-02-08</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5p7ujcrUXASCNwRaWNHR1C</td>\n      <td>Without Me</td>\n      <td>{'26VFTg2z8YR0cCuwLzESi2': 'Halsey'}</td>\n      <td>87</td>\n      <td>True</td>\n      <td>2018-10-04</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2xLMifQCjDGFmkHkpNLD9h</td>\n      <td>SICKO MODE</td>\n      <td>{'0Y5tJX1MQlPlqiwlOH1tJY': 'Travis Scott'}</td>\n      <td>85</td>\n      <td>True</td>\n      <td>2018-08-03</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3KkXRkHbMCARz0aVfEt68P</td>\n      <td>Sunflower - Spider-Man: Into the Spider-Verse</td>\n      <td>{'246dkjvS1zLTtiykXe5h60': 'Post Malone', '1zN...</td>\n      <td>92</td>\n      <td>False</td>\n      <td>2018-12-14</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1rqqCSm0Qe4I9rUvWncaom</td>\n      <td>High Hopes</td>\n      <td>{'20JZFwl6HVl6yg8a4H3ZqK': 'Panic! At The Disco'}</td>\n      <td>86</td>\n      <td>False</td>\n      <td>2018-06-22</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>20400</th>\n      <td>4NnhLA66RRLXxKbiiscU9R</td>\n      <td>How Can I Be Sure - Single Version</td>\n      <td>{'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}</td>\n      <td>36</td>\n      <td>False</td>\n      <td>1967</td>\n      <td>year</td>\n    </tr>\n    <tr>\n      <th>20401</th>\n      <td>2jHfXdCLibrI1J56LnUAZv</td>\n      <td>To Show I Love You - Mono</td>\n      <td>{'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}</td>\n      <td>1</td>\n      <td>False</td>\n      <td>2009-11-27</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20402</th>\n      <td>6zqsyB7uIvWrL1iCJzpNrs</td>\n      <td>You Better Run - Single Version</td>\n      <td>{'5X3TuTi9OIsJXMGxPwTKM2': 'The Young Rascals'}</td>\n      <td>20</td>\n      <td>False</td>\n      <td>1967</td>\n      <td>year</td>\n    </tr>\n    <tr>\n      <th>20403</th>\n      <td>5mz9pQZZXNpAw9CdQ7Bk8q</td>\n      <td>Don't Pity Me - Mono</td>\n      <td>{'6lHC2EQMEMZiEmSfFloarn': 'Peter And Gordon'}</td>\n      <td>7</td>\n      <td>False</td>\n      <td>2009-11-27</td>\n      <td>day</td>\n    </tr>\n    <tr>\n      <th>20404</th>\n      <td>2H9CKpZiLDF223BbwehpDF</td>\n      <td>Hush Hush</td>\n      <td>{'6wPhSqRtPu1UhRCDX5yaDJ': 'The Pussycat Dolls'}</td>\n      <td>57</td>\n      <td>False</td>\n      <td>2009-01-01</td>\n      <td>day</td>\n    </tr>\n  </tbody>\n</table>\n<p>20405 rows × 7 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.drop(columns=[\"billboard\", \"song_type\", \"album_id\", \"track_number\"], inplace=True)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3080c8b0d1edac85",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:07.444072400Z",
     "start_time": "2024-02-21T16:27:07.325075600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                    artist_id             name   followers  popularity  \\\n0      66CXWjxzNUsdJxJ2JdwvnR    Ariana Grande  34554242.0          96   \n1      26VFTg2z8YR0cCuwLzESi2           Halsey   7368242.0          90   \n2      0Y5tJX1MQlPlqiwlOH1tJY     Travis Scott   6313709.0          94   \n3      246dkjvS1zLTtiykXe5h60      Post Malone  16737002.0          96   \n4      1zNqQNIdeOUZHb8zbZRFMX         Swae Lee    483032.0          89   \n...                       ...              ...         ...         ...   \n11513  7vyRisgvM6Wm0Pnp0qXx6m     Sweeney Todd       634.0          19   \n11514  2Uh4UmiQhrrElbrvJVH0dT  Brooklyn Dreams       318.0          32   \n11515  1VGFS4UGLOAxlMGqzcqHG1              PMD      1405.0          32   \n11516  3Se8xpgCBmfXVnZqRSRRH9  The Tribute Co.       274.0          21   \n11517  45d3pteh2TnzUMMl27J4MY     Tommy Morgan        78.0          15   \n\n                                                  genres  \n0                  ['dance pop', 'pop', 'post-teen pop']  \n1      ['dance pop', 'electropop', 'etherpop', 'indie...  \n2                              ['pop', 'pop rap', 'rap']  \n3                              ['dfw rap', 'pop', 'rap']  \n4                                         ['trap music']  \n...                                                  ...  \n11513                          ['classic canadian rock']  \n11514                                                 []  \n11515                                                 []  \n11516                                                 []  \n11517                                                 []  \n\n[11518 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist_id</th>\n      <th>name</th>\n      <th>followers</th>\n      <th>popularity</th>\n      <th>genres</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>66CXWjxzNUsdJxJ2JdwvnR</td>\n      <td>Ariana Grande</td>\n      <td>34554242.0</td>\n      <td>96</td>\n      <td>['dance pop', 'pop', 'post-teen pop']</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26VFTg2z8YR0cCuwLzESi2</td>\n      <td>Halsey</td>\n      <td>7368242.0</td>\n      <td>90</td>\n      <td>['dance pop', 'electropop', 'etherpop', 'indie...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0Y5tJX1MQlPlqiwlOH1tJY</td>\n      <td>Travis Scott</td>\n      <td>6313709.0</td>\n      <td>94</td>\n      <td>['pop', 'pop rap', 'rap']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>246dkjvS1zLTtiykXe5h60</td>\n      <td>Post Malone</td>\n      <td>16737002.0</td>\n      <td>96</td>\n      <td>['dfw rap', 'pop', 'rap']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1zNqQNIdeOUZHb8zbZRFMX</td>\n      <td>Swae Lee</td>\n      <td>483032.0</td>\n      <td>89</td>\n      <td>['trap music']</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11513</th>\n      <td>7vyRisgvM6Wm0Pnp0qXx6m</td>\n      <td>Sweeney Todd</td>\n      <td>634.0</td>\n      <td>19</td>\n      <td>['classic canadian rock']</td>\n    </tr>\n    <tr>\n      <th>11514</th>\n      <td>2Uh4UmiQhrrElbrvJVH0dT</td>\n      <td>Brooklyn Dreams</td>\n      <td>318.0</td>\n      <td>32</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>11515</th>\n      <td>1VGFS4UGLOAxlMGqzcqHG1</td>\n      <td>PMD</td>\n      <td>1405.0</td>\n      <td>32</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>11516</th>\n      <td>3Se8xpgCBmfXVnZqRSRRH9</td>\n      <td>The Tribute Co.</td>\n      <td>274.0</td>\n      <td>21</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>11517</th>\n      <td>45d3pteh2TnzUMMl27J4MY</td>\n      <td>Tommy Morgan</td>\n      <td>78.0</td>\n      <td>15</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n<p>11518 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artists = pd.read_csv(\"musicoset_metadata/artists.csv\", delimiter=\"\\t\")\n",
    "df_artists.drop(columns=[\"artist_type\", \"main_genre\", \"image_url\"], inplace=True)\n",
    "df_artists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f830347107cf22",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T16:27:32.903854Z",
     "start_time": "2024-02-21T16:27:07.409075Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize a new graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for _, row in merged_df.iterrows():\n",
    "    # Get the list of artist IDs from the dictionary\n",
    "    artist_ids = list(row['artists'].keys())\n",
    "    # Add nodes with additional attributes\n",
    "    for artist_id in artist_ids:\n",
    "        # Extract additional information about the artist from the DataFrame\n",
    "        artist_info = df_artists[df_artists[\"artist_id\"] == artist_id]\n",
    "        # Add the node with additional attributes\n",
    "        G.add_node(artist_id, name=artist_info['name'], popularity=artist_info['popularity'], followers=artist_info[\"followers\"], genres=artist_info[\"genres\"])\n",
    "    # Add edges between all pairs of artists and update their weights\n",
    "    for i in range(len(artist_ids)):\n",
    "        if len(artist_ids[i]) == 1:\n",
    "            if G.has_edge(artist_ids[0], artist_ids[0]):\n",
    "                # If the edge already exists, increment the weight by 1\n",
    "                G[artist_ids[i]][artist_ids[0]]['weight'] += 1\n",
    "        else:\n",
    "            # Add a new edge with weight 1\n",
    "            G.add_edge(artist_ids[0], artist_ids[0], weight=1, song_id=row[\"song_id\"], song_name=row[\"song_name\"], explicit=row[\"explicit\"], popularity=row[\"popularity\"], release_date=row[\"release_date\"], release_date_precision=row[\"release_date_precision\"])\n",
    "        for j in range(i + 1, len(artist_ids)):\n",
    "            artist_id_1 = artist_ids[i]\n",
    "            artist_id_2 = artist_ids[j]\n",
    "            # Check if the edge already exists\n",
    "            if G.has_edge(artist_id_1, artist_id_2):\n",
    "                # If the edge already exists, increment the weight by 1\n",
    "                G[artist_id_1][artist_id_2]['weight'] += 1\n",
    "            #elif G.has_edge(artist_id_2, artist_id_1):\n",
    "            #G[artist_id_2][artist_id_1]['weight'] += 1\n",
    "            else:\n",
    "                # Add a new edge with weight 1\n",
    "                G.add_edge(artist_id_1, artist_id_2, weight=1, song_id=row[\"song_id\"], song_name=row[\"song_name\"], explicit=row[\"explicit\"], popularity=row[\"popularity\"], release_date=row[\"release_date\"], release_date_precision=row[\"release_date_precision\"])\n",
    "\n",
    "# Now, G is a bidirectional weighted graph where each edge represents a collaboration\n",
    "# between two artists, and the weight represents the number of collaborations.\n",
    "# Nodes also have additional attributes such as 'name' and 'popularity'.\n",
    "\n",
    "\n",
    "#is two sided adding necessary\n",
    "#with solo songs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e7201820da355",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-21T16:27:32.905853400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Draw the graph\n",
    "nx.draw(G, with_labels=False, node_color='skyblue', node_size=1, edge_color='black', linewidths=1, font_size=15)\n",
    "\n",
    "# Display the plot\n",
    "plt.savefig(\"network.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f695ebd3500fa5df",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## About the Network\n",
    "\n",
    "comments:\n",
    "- use names instead\n",
    "- use other direction\n",
    "- use only popular artists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343db669743e392a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "num_nodes = G.number_of_nodes()\n",
    "num_edges = G.number_of_edges()\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eee6750d355e94",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "node_list = list(G.nodes())\n",
    "edge_list = list(G.edges())\n",
    "#print(\"List of nodes:\", node_list)\n",
    "#print(\"List of edges:\", edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196fcd4b2ece1587",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "degree = G.degree()\n",
    "print(\"Degree of nodes:\", degree)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3108ddef234b0d64",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "density = nx.density(G)\n",
    "print(\"Graph density:\", density)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a465cde15a932a7b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Calculate the Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f603e13d5c0d8f63",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# (1) Interaction (degree and weighted degree)\n",
    "# (a) Single artists collaboration\n",
    "# Calculate degree for single artists\n",
    "single_artist_degree = dict(G.degree())\n",
    "\n",
    "# (b) Multi-artists collaboration\n",
    "# Calculate weighted degree for multi-artists collaboration\n",
    "multi_artist_degree = dict(G.degree(weight='weight'))\n",
    "\n",
    "# (2) Distance (closeness and eccentricity)\n",
    "# Calculate closeness centrality\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "\n",
    "# Calculate eccentricity\n",
    "# Check if the graph is connected\n",
    "if not nx.is_connected(G):\n",
    "    # Get the connected components\n",
    "    components = nx.connected_components(G)\n",
    "    for component in components:\n",
    "        # Create subgraphs for each connected component\n",
    "        subgraph = G.subgraph(component)\n",
    "        # Now, you can analyze each subgraph separately\n",
    "        # For example, you can calculate metrics like degree centrality for each subgraph\n",
    "        degree_centrality = nx.degree_centrality(subgraph)\n",
    "        nx.set_node_attributes(G, degree_centrality, name='degree_centrality')\n",
    "        print(\"Degree Centrality for component:\", degree_centrality)\n",
    "else:        \n",
    "    eccentricity = nx.eccentricity(G)\n",
    "\n",
    "# (3) Similarity (clustering coefficient)\n",
    "# Calculate clustering coefficient\n",
    "clustering_coefficient = nx.clustering(G)\n",
    "\n",
    "# (4) Influence (betweenness and eigenvector)\n",
    "# Calculate betweenness centrality\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "# Calculate eigenvector centrality\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bd0b7a0cb4233c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Add degree centrality as a node attribute\n",
    "#nx.set_node_attributes(G, degree_centrality, name='degree_centrality')\n",
    "\n",
    "# Add degree and weighted degree as node attributes\n",
    "nx.set_node_attributes(G, single_artist_degree, name='degree')\n",
    "nx.set_node_attributes(G, multi_artist_degree, name='weighted_degree')\n",
    "\n",
    "# Add closeness centrality as a node attribute\n",
    "nx.set_node_attributes(G, closeness_centrality, name='closeness_centrality')\n",
    "\n",
    "# Add clustering coefficient as a node attribute\n",
    "nx.set_node_attributes(G, clustering_coefficient, name='clustering_coefficient')\n",
    "\n",
    "# Add betweenness centrality as a node attribute\n",
    "nx.set_node_attributes(G, betweenness_centrality, name='betweenness_centrality')\n",
    "\n",
    "# Add eigenvector centrality as a node attribute\n",
    "nx.set_node_attributes(G, eigenvector_centrality, name='eigenvector_centrality')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b926ea6b5e9131f9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a79cd3bbda3a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "now try clustering based on metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd08e2703d125ae",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming G is your NetworkX graph\n",
    "\n",
    "# Extract nodes and their attributes from the graph\n",
    "nodes_with_attributes = [(node, G.nodes[node]) for node in G.nodes()]\n",
    "\n",
    "# Create a DataFrame from the nodes and their attributes\n",
    "df_nodes = pd.DataFrame(nodes_with_attributes, columns=['Node', 'Attributes'])\n",
    "\n",
    "# Normalize the 'Attributes' column to expand it into separate columns\n",
    "df_attributes = pd.json_normalize(df_nodes['Attributes'])\n",
    "\n",
    "# Combine the original 'Node' column with the expanded attributes\n",
    "df_nodes = pd.concat([df_nodes['Node'], df_attributes], axis=1)\n",
    "\n",
    "# Display the DataFrame\n",
    "df_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_nodes"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "2f07ab046b4f4bf7"
  },
  {
   "cell_type": "markdown",
   "id": "28230451d530a6a4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "normalize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9dac0f54e92674",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming df_nodes is your DataFrame\n",
    "# Select only the columns to be normalized\n",
    "columns_to_normalize = df_nodes.columns[1:-1]  # Exclude the first and last columns\n",
    "data_to_normalize = df_nodes[columns_to_normalize]\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "normalized_data = scaler.fit_transform(data_to_normalize)\n",
    "\n",
    "# Replace the original data with the normalized values\n",
    "df_nodes_norm = df_nodes.copy()\n",
    "df_nodes_norm[columns_to_normalize] = normalized_data\n",
    "\n",
    "# Print the updated DataFrame\n",
    "df_nodes_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4e01f53b72db10",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "#print max value for each coloumn\n",
    "print(df_nodes.max())\n",
    "print(df_nodes_norm.max())\n",
    "\n",
    "#print min value for each coloumn\n",
    "print(df_nodes.min())\n",
    "print(df_nodes_norm.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8547624cd7e138e",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame with node attributes and edge attributes added\n",
    "\n",
    "# Convert the DataFrame to a sparse adjacency matrix\n",
    "A_dense = pairwise_distances(df_nodes_norm.drop(columns=['Node']), metric='euclidean')\n",
    "\n",
    "# Apply Louvain community detection algorithm to detect communities (if needed)\n",
    "# You may need to find an alternative community detection algorithm for DataFrame-based graphs\n",
    "\n",
    "# Calculate the modularity for different values of k\n",
    "k_values = range(1, 11)  # Adjust the range as needed\n",
    "inertias = []\n",
    "\n",
    "amount_k = len(k_values)\n",
    "for k in k_values:\n",
    "    print(f'{k}/{amount_k}')\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(A_dense)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b13e8bf4d3c834d",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "\n",
    "# Assuming df is your DataFrame with node attributes and edge attributes added\n",
    "\n",
    "# Convert the DataFrame to a sparse adjacency matrix\n",
    "#A_dense = pairwise_distances(df_nodes.drop(columns=['Node']), metric='euclidean')\n",
    "A_dense = df_nodes_norm.drop(columns=['Node'])\n",
    "\n",
    "# Apply Louvain community detection algorithm to detect communities (if needed)\n",
    "# You may need to find an alternative community detection algorithm for DataFrame-based graphs\n",
    "\n",
    "# Calculate the modularity for different values of k\n",
    "k_values = range(1, 11)  # Adjust the range as needed\n",
    "inertias = []\n",
    "\n",
    "amount_k = len(k_values)\n",
    "for k in k_values:\n",
    "    print(f'{k}/{amount_k}')\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(A_dense)\n",
    "    inertia = kmeans.inertia_\n",
    "    inertias.append(inertia)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, inertias, marker='o')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fe0307aa86161",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "for unnormalized the results were 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b6c45eb23bcd8",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming df_nodes is your DataFrame containing node attributes\n",
    "\n",
    "# Extract the features (attributes) from the DataFrame\n",
    "X = df_nodes.drop(columns=['Node'])\n",
    "\n",
    "# Specify the number of clusters (k)\n",
    "k = 3  # Adjust as needed\n",
    "\n",
    "# Initialize KMeans model\n",
    "kmeans = KMeans(n_clusters=k)\n",
    "\n",
    "# Fit the model to the data\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Get the cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels as a new column in the DataFrame\n",
    "df_nodes['Cluster'] = cluster_labels\n",
    "df_nodes_norm['Cluster'] = cluster_labels\n",
    "\n",
    "# Display the DataFrame with cluster labels\n",
    "df_nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8680c80a77d6177d",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "# Assuming df_nodes is your DataFrame with cluster labels\n",
    "\n",
    "# Calculate the cardinality of each cluster\n",
    "cluster_cardinality = df_nodes_norm['Cluster'].value_counts()\n",
    "\n",
    "# Display the cardinality of each cluster\n",
    "print(cluster_cardinality)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3749069934c9ce05",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "##Dimensional reduction:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "\n",
    "# Omit the first and last columns\n",
    "X = df_nodes.iloc[:, 1:-1]\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "df_2d = pd.DataFrame(X_2d, columns=['Component 1', 'Component 2'])\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df_2d[\"Node\"] = df_nodes[\"Node\"]\n",
    "df_2d[\"Cluster\"] = df_nodes[\"Cluster\"]\n",
    "df_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5972d313887145",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df_nodes is your DataFrame with cluster labels\n",
    "\n",
    "# Plot the clusters in a scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Iterate over unique cluster labels\n",
    "for cluster_label in df_2d['Cluster'].unique():\n",
    "    # Filter the DataFrame for nodes in the current cluster\n",
    "    df_cluster = df_2d[df_2d['Cluster'] == cluster_label]\n",
    "    # Plot the nodes in the cluster\n",
    "    plt.scatter(df_cluster[\"Component 1\"], df_cluster[\"Component 2\"], s=10, label=f'Cluster {cluster_label}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Attribute1')\n",
    "plt.ylabel('Attribute2')\n",
    "plt.title('Clusters Visualization')\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df81acc570751f8c",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the attributes for the radar chart\n",
    "#get coloumn names\n",
    "attributes = df_nodes_norm.iloc[:, 1:-1].columns.tolist()\n",
    "\n",
    "# Create a figure and add a subplot with polar projection\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Define the theta values (angles) for each attribute\n",
    "theta = np.linspace(0, 2 * np.pi, len(attributes), endpoint=False).tolist()\n",
    "\n",
    "# Define the values for the instance\n",
    "# Replace the values below with your actual instance values\n",
    "cluster_data = df_nodes_norm[df_nodes_norm['Cluster'] == 0]\n",
    "\n",
    "# Get attribute values for the current cluster\n",
    "#values = cluster_data.iloc[0, 1:-1].tolist()  # Exclude the cluster and name columns, assuming only one row for each cluster\n",
    "values = cluster_data.iloc[:, 1:-1].mean(axis=0).tolist()\n",
    "\n",
    "\n",
    "# Repeat the first value to close the plot\n",
    "#values = values + values[:1]\n",
    "\n",
    "# Plot the radar chart\n",
    "ax.fill(theta, values, color='blue', alpha=0.25)\n",
    "ax.plot(theta, values, color='blue', linewidth=2)\n",
    "\n",
    "# Set the labels for each attribute\n",
    "ax.set_xticks(theta)\n",
    "ax.set_xticklabels(attributes)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beae62c65cd64e3",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the attributes for the radar chart\n",
    "attributes = df_nodes.iloc[:, 1:-1].columns.tolist()\n",
    "\n",
    "# Define the theta values (angles) for each attribute\n",
    "theta = np.linspace(0, 2 * np.pi, len(attributes), endpoint=False)\n",
    "\n",
    "# Iterate over each unique cluster label\n",
    "for cluster_label in df_nodes_norm['Cluster'].unique():\n",
    "    # Create a figure and add a subplot with polar projection for each cluster\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Filter the DataFrame for the current cluster\n",
    "    cluster_data = df_nodes[df_nodes['Cluster'] == cluster_label]\n",
    "\n",
    "    # Get attribute values for the current cluster\n",
    "    values = cluster_data.iloc[:, 1:-1].mean(axis=0).tolist()\n",
    "\n",
    "    # Repeat the first value to close the plot\n",
    "    #values += values[:1]\n",
    "\n",
    "    # Plot the radar chart\n",
    "    ax.fill(theta, values, color='blue', alpha=0.25)\n",
    "    ax.plot(theta, values, color='blue', linewidth=2)\n",
    "\n",
    "    # Set the labels for each attribute\n",
    "    ax.set_xticks(theta)\n",
    "    ax.set_xticklabels(attributes)\n",
    "\n",
    "    # Set the title for the radar chart\n",
    "    ax.set_title(f'Cluster {cluster_label} Radar Chart')\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b25725b2ec7116",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d8df6e2c27292c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "all makes more sense for not norm? \n",
    "=> take a look at init of graph and change?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b296c0fb734af9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "create another plot of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31b460c11833416",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "liste = range(2,1)\n",
    "liste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251111b5f8ba89d",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Merge DataFrames to get pairs of artists for each song\n",
    "df_collaborations = df_songs.merge(df_artists, on='artist_id', how='inner')\n",
    "\n",
    "# Count the number of songs each pair of artists has collaborated on\n",
    "collaboration_counts = df_collaborations.groupby(['artist_name'])['artist_name'].count().reset_index(name='count')\n",
    "\n",
    "# Create an empty graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add artists as nodes to the graph\n",
    "for _, row in collaboration_counts.iterrows():\n",
    "    G.add_node(row['artist_name'])\n",
    "\n",
    "# Iterate over each pair of artists and add weighted edges to the graph\n",
    "for i in range(len(collaboration_counts)):\n",
    "    for j in range(i + 1, len(collaboration_counts)):\n",
    "        artist1 = collaboration_counts.loc[i, 'artist_name']\n",
    "        artist2 = collaboration_counts.loc[j, 'artist_name']\n",
    "        common_songs = df_collaborations[(df_collaborations['artist_name'] == artist1) & (df_collaborations['artist_name'] == artist2)].shape[0]\n",
    "        if common_songs > 0:\n",
    "            G.add_edge(artist1, artist2, weight=common_songs)\n",
    "\n",
    "# Now G is an undirected graph where the weight of the edge between two artists \n",
    "# represents the number of songs they have collaborated on"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
